#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Goliath Auto Tool System - main.py (single-file)

- 8-hour cycle runner (run once; scheduling is outside)
- Collect from: Bluesky, Mastodon, Reddit, Hacker News, X (mentions)
- Normalize -> cluster -> choose best themes -> generate solution sites
- Validate/autofix up to MAX_AUTOFIX
- Update hub/sites.json only (hub assets + hub/index.html are frozen)
- Generate sitemap.xml + robots.txt safely (default: goliath/_out; root only if ALLOW_ROOT_UPDATE=1)
- Generate shortlinks (/goliath/go/<code>/) and provide "short URL + one-line value" post drafts
- Output Issues payload (bulk) with:
    - Problem URL
    - Reply (EN, empathy + ‚Äúmade a one-page guide‚Äù + tool URL last line)
  Minimum 100 leads per run (stub fill if needed)
- 22 categories (fixed) -> affiliates.json top2 -> inject to AFF_SLOT
- SaaS-like design, Tailwind, dark mode, i18n (EN/JA/KO/ZH), 2500+ chars JP article + FAQ + references + legal pages
"""

from __future__ import annotations

import base64
import datetime as dt
import hashlib
import html
import json
import logging
import os
import random
import re
import sys
import time
from dataclasses import dataclass
from typing import Any, Dict, Iterable, List, Optional, Tuple
from urllib.error import HTTPError, URLError
from urllib.parse import quote, urlencode, urlparse
from urllib.request import Request, urlopen


# =============================================================================
# Config (ENV)
# =============================================================================
def env_first(*names: str, default: str = "") -> str:
    for n in names:
        v = os.environ.get(n, "").strip()
        if v:
            return v
    return default

# ---- Public base (linkÁîüÊàê„ÅØ„Åì„ÅìÂü∫Ê∫ñ) ----
# ‰ªä„ÅØ GitHub Pages ÈÖç‰∏ã„Å´Âá∫„Åó„Åü„ÅÑ ‚Üí ActionsÂÅ¥„Åß PUBLIC_BASE_URL „ÇíÂÖ•„Çå„Çã
# ‰æã: https://mikann20041029.github.io
PUBLIC_BASE_URL = env_first("PUBLIC_BASE_URL", "PUBLIC_SITE_BASE", default=os.environ.get("SITE_DOMAIN", "").strip() or "https://mikann20041029.github.io")

# ---- Bluesky ----
BLUESKY_HANDLE = env_first("BLUESKY_HANDLE", "BSKY_HANDLE", "BLUESKY_ID")
BLUESKY_APP_PASSWORD = env_first("BLUESKY_APP_PASSWORD", "BSKY_APP_PASSWORD", "BLUESKY_PASSWORD")

# ---- Mastodon ----
MASTODON_BASE = env_first("MASTODON_BASE", "MASTODON_INSTANCE", "MASTODON_URL")
MASTODON_TOKEN = env_first("MASTODON_TOKEN", "MASTODON_ACCESS_TOKEN")

# ---- Reddit ----
REDDIT_CLIENT_ID = env_first("REDDIT_CLIENT_ID", "REDDIT_ID")
REDDIT_CLIENT_SECRET = env_first("REDDIT_CLIENT_SECRET", "REDDIT_SECRET")
REDDIT_REFRESH_TOKEN = env_first("REDDIT_REFRESH_TOKEN", "REDDIT_REFRESH")
REDDIT_USER_AGENT = env_first("REDDIT_USER_AGENT", default="goliath-tool/1.0 (read-only)")

# ---- X (Free: Êúà100 Reads ÊÉ≥ÂÆö / 1ÂÆüË°å=1„É™„ÇØ„Ç®„Çπ„ÉàÈÅãÁî®) ----
X_BEARER_TOKEN = env_first("X_BEARER_TOKEN", "TWITTER_BEARER_TOKEN", "X_TOKEN", "TW_BEARER_TOKEN")
X_SEARCH_QUERY = os.environ.get("X_SEARCH_QUERY", '("how to" OR help OR error OR fix) lang:en -is:retweet').strip()
X_MAX = int(os.environ.get("X_MAX", "1"))
def getenv_any(names: Iterable[str], default: str = "") -> str:
    for n in names:
        v = os.environ.get(n)
        if v is None:
            continue
        v = str(v).strip()
        if v:
            return v
    return default

HN_QUERY = getenv_any(["HN_QUERY", "HACKER_NEWS_QUERY", "HN_SEARCH_QUERY"], "how to fix error OR help OR cannot OR failed OR bug")
# ---- Hacker News ----
HN_MAX = int(os.environ.get("HN_MAX", os.environ.get("HACKER_NEWS_MAX", "70")))

# ---- State file (ÈáçË§áËøî‰ø°Èò≤Ê≠¢) ----


REPO_ROOT = os.environ.get("REPO_ROOT", os.getcwd())

STATE_DIR = os.path.join(REPO_ROOT, "state")
LAST_SEEN_PATH = os.path.join(STATE_DIR, "last_seen.json")


GOLIATH_DIR = os.path.join(REPO_ROOT, "goliath")
PAGES_DIR = os.path.join(GOLIATH_DIR, "pages")
OUT_DIR = os.path.join(GOLIATH_DIR, "_out")  # safe outputs (sitemap/robots/issues payload, etc.)

POLICIES_DIR = os.path.join(REPO_ROOT, "policies")  # allowed by your rule (new folder)
HUB_DIR = os.path.join(REPO_ROOT, "hub")
HUB_SITES_JSON = os.path.join(HUB_DIR, "sites.json")

AFFILIATES_JSON = os.environ.get("AFFILIATES_JSON", os.path.join(REPO_ROOT, "affiliates.json"))

DEFAULT_LANG = os.environ.get("DEFAULT_LANG", "en")  # en/ja/ko/zh
LANGS = ["en", "ja", "ko", "zh"]

RUN_ID = os.environ.get("RUN_ID", str(int(time.time())))
RANDOM_SEED = os.environ.get("RANDOM_SEED", RUN_ID)

MAX_THEMES = int(os.environ.get("MAX_THEMES", "6"))           # how many sites to build per run
MAX_COLLECT = int(os.environ.get("MAX_COLLECT", "260"))       # total target; spec 173+; overshoot allowed
MAX_AUTOFIX = int(os.environ.get("MAX_AUTOFIX", "5"))

ALLOW_ROOT_UPDATE = os.environ.get("ALLOW_ROOT_UPDATE", "0") == "1"
PING_SITEMAP = os.environ.get("PING_SITEMAP", "0") == "1"

# Social API credentials (optional)
# Social API credentials (optional) - accept alias env names too
BLUESKY_HANDLE = getenv_any(["BLUESKY_HANDLE", "BSKY_HANDLE", "BLUESKY_ID"], "")
BLUESKY_APP_PASSWORD = getenv_any(["BLUESKY_APP_PASSWORD", "BSKY_APP_PASSWORD", "BLUESKY_PASSWORD"], "")

MASTODON_BASE = getenv_any(["MASTODON_BASE", "MASTODON_INSTANCE", "MASTODON_INSTANCE_URL"], "")  # e.g. https://mastodon.social
MASTODON_TOKEN = getenv_any(["MASTODON_TOKEN", "MASTODON_ACCESS_TOKEN", "MASTODON_BEARER_TOKEN"], "")

REDDIT_CLIENT_ID = getenv_any(["REDDIT_CLIENT_ID", "REDDIT_ID", "REDDIT_APP_ID"], "")
REDDIT_CLIENT_SECRET = getenv_any(["REDDIT_CLIENT_SECRET", "REDDIT_SECRET", "REDDIT_APP_SECRET"], "")
REDDIT_REFRESH_TOKEN = getenv_any(["REDDIT_REFRESH_TOKEN", "REDDIT_TOKEN"], "")
REDDIT_USER_AGENT = getenv_any(["REDDIT_USER_AGENT"], "goliath-tool/1.0 (read-only)")
REDDIT_SUBREDDITS = getenv_any(["REDDIT_SUBREDDITS", "REDDIT_SUBS", "SUBREDDITS"], "webdev,sysadmin,programming,techsupport,github,privacy,excel,personalfinance,travel,solotravel,productivity,studytips,mealprep,fitness")

# X (Twitter) ‚Äî accept alias + allow keyword-search mode
X_BEARER_TOKEN = getenv_any([
    "X_BEARER_TOKEN",
    "X_BEARER",
    "TWITTER_BEARER_TOKEN",
    "TW_BEARER_TOKEN",
    "X_API_BEARER_TOKEN",
    "TWITTER_API_BEARER_TOKEN",
], "")
X_USER_ID = getenv_any(["X_USER_ID", "X_USERID", "TWITTER_USER_ID", "TW_USER_ID", "TWITTER_ID", "X_ID"], "")
X_QUERY = getenv_any(["X_QUERY", "X_SEARCH_QUERY", "TWITTER_QUERY"], "")
X_API_BASE = getenv_any(["X_API_BASE", "TWITTER_API_BASE"], "https://api.x.com")

X_MAX = int(os.environ.get("X_MAX", "1"))    # 1 run = 1Êé°Áî®ÔºàreadÁØÄÁ¥Ñ„ÅÆÂâçÊèêÔºâ


# OpenAI (optional) - not required; kept off by default in this file
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")
OPENAI_MODEL = os.environ.get("OPENAI_MODEL", "gpt-4.1-mini")

# Content requirements
MIN_ARTICLE_CHARS_JA = int(os.environ.get("MIN_ARTICLE_CHARS_JA", "2500"))
MIN_FAQ = int(os.environ.get("MIN_FAQ", "5"))
REF_URL_MIN = int(os.environ.get("REF_URL_MIN", "10"))
REF_URL_MAX = int(os.environ.get("REF_URL_MAX", "20"))
SUPP_URL_MIN = int(os.environ.get("SUPP_URL_MIN", "3"))

# Leads/Issues
issue_items: List[Dict[str, Any]] = []
reply_count = 0

LEADS_TOTAL = int(os.environ.get("LEADS_TOTAL", "100"))  # IMPORTANT: default 100 per your requirement
ISSUE_MAX_ITEMS = int(os.environ.get("ISSUE_MAX_ITEMS", "40"))  # chunking for long issue body

# Branding / canonical
# Branding / canonical
SITE_BRAND = os.environ.get("SITE_BRAND", "Mikanntool")
SITE_LOGO = os.environ.get("SITE_LOGO", "üß∞")

SITE_DOMAIN = env_first("SITE_DOMAIN", default=PUBLIC_BASE_URL)
HUB_BASE_URL = env_first("HUB_BASE_URL", default=PUBLIC_BASE_URL.rstrip("/") + "/hub/")


SITE_CONTACT_EMAIL = os.environ.get("SITE_CONTACT_EMAIL", "contact@mikanntool.com")


# Unsplash (optional): if set, we fetch one photo URL for hero background
UNSPLASH_ACCESS_KEY = os.environ.get("UNSPLASH_ACCESS_KEY", "")

# Keep hub frozen: do not touch these
FROZEN_PATH_PREFIXES = [
    os.path.join(REPO_ROOT, "hub", "index.html"),
    os.path.join(REPO_ROOT, "hub", "assets"),
    os.path.join(REPO_ROOT, "hub", "assets", "ui.v3.css"),
    os.path.join(REPO_ROOT, "hub", "assets", "app.v3.js"),
]

# 22 categories fixed (your spec)
CATEGORIES_22 = [
    "Web/Hosting",
    "Dev/Tools",
    "AI/Automation",
    "Security/Privacy",
    "Media",
    "PDF/Docs",
    "Images/Design",
    "Data/Spreadsheets",
    "Business/Accounting/Tax",
    "Marketing/Social",
    "Productivity",
    "Education/Language",
    "Travel/Planning",
    "Food/Cooking",
    "Health/Fitness",
    "Study/Learning",
    "Money/Personal Finance",
    "Career/Work",
    "Relationships/Communication",
    "Home/Life Admin",
    "Shopping/Products",
    "Events/Leisure",
]

random.seed(int(hashlib.sha256(RANDOM_SEED.encode("utf-8")).hexdigest()[:8], 16))


# =============================================================================
# Logging
# =============================================================================
def setup_logging() -> None:
    os.makedirs(OUT_DIR, exist_ok=True)
    log_path = os.path.join(OUT_DIR, f"run_{RUN_ID}.log")
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s | %(levelname)s | %(message)s",
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler(log_path, encoding="utf-8"),
        ],
    )
    logging.info("RUN_ID=%s", RUN_ID)
    logging.info("REPO_ROOT=%s", REPO_ROOT)
    logging.info("SITE_DOMAIN=%s", SITE_DOMAIN)
    logging.info("ALLOW_ROOT_UPDATE=%s", ALLOW_ROOT_UPDATE)
    logging.info("LEADS_TOTAL=%s", LEADS_TOTAL)
    logging.info("HN_QUERY=%s", HN_QUERY)
    logging.info("HN_MAX=%s", HN_MAX)



# =============================================================================
# Utilities (IO / HTTP / Text)
# =============================================================================
def read_text(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def write_text(path: str, content: str) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8", newline="\n") as f:
        f.write(content)


def read_json(path: str, default: Any = None) -> Any:
    if not os.path.exists(path):
        return default
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def write_json(path: str, obj: Any) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8", newline="\n") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)


def now_iso() -> str:
    return dt.datetime.now(dt.timezone.utc).astimezone().isoformat(timespec="seconds")


def sha1(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8")).hexdigest()


def safe_slug(s: str, maxlen: int = 64) -> str:
    s = (s or "").strip().lower()
    s = re.sub(r"https?://", "", s)
    s = re.sub(r"[^a-z0-9]+", "-", s)
    s = re.sub(r"-{2,}", "-", s).strip("-")
    if not s:
        s = "tool"
    return (s[:maxlen].strip("-") or "tool")


def clamp(n: int, lo: int, hi: int) -> int:
    return max(lo, min(hi, n))


def uniq_keep_order(items: List[str]) -> List[str]:
    seen = set()
    out: List[str] = []
    for x in items:
        if x not in seen:
            out.append(x)
            seen.add(x)
    return out


def is_frozen_path(path: str) -> bool:
    p = os.path.abspath(path)
    for fp in FROZEN_PATH_PREFIXES:
        fp_abs = os.path.abspath(fp)
        if p == fp_abs:
            return True
        if os.path.isdir(fp_abs) and p.startswith(fp_abs + os.sep):
            return True
    return False


def http_get(url: str, headers: Optional[Dict[str, str]] = None, timeout: int = 20) -> Tuple[int, str]:
    h = headers or {}
    req = Request(url, headers=h, method="GET")
    try:
        with urlopen(req, timeout=timeout) as resp:
            status = resp.status
            data = resp.read()
            try:
                text = data.decode("utf-8")
            except UnicodeDecodeError:
                text = data.decode("utf-8", errors="replace")
            return status, text
    except HTTPError as e:
        try:
            body = e.read().decode("utf-8", errors="replace")
        except Exception:
            body = ""
        return e.code, body
    except URLError as e:
        return 0, str(e)


def http_post_json(
    url: str,
    payload: Dict[str, Any],
    headers: Optional[Dict[str, str]] = None,
    timeout: int = 20,
) -> Tuple[int, Dict[str, Any], str]:
    h = {"Content-Type": "application/json"}
    if headers:
        h.update(headers)
    data = json.dumps(payload).encode("utf-8")
    req = Request(url, headers=h, data=data, method="POST")
    try:
        with urlopen(req, timeout=timeout) as resp:
            status = resp.status
            raw = resp.read().decode("utf-8", errors="replace")
            try:
                return status, json.loads(raw), raw
            except Exception:
                return status, {}, raw
    except HTTPError as e:
        raw = ""
        try:
            raw = e.read().decode("utf-8", errors="replace")
        except Exception:
            pass
        try:
            return e.code, json.loads(raw), raw
        except Exception:
            return e.code, {}, raw
    except URLError as e:
        return 0, {}, str(e)


def base64_basic_auth(user: str, password: str) -> str:
    token = f"{user}:{password}"
    return base64.b64encode(token.encode("utf-8")).decode("ascii")


# =============================================================================
# Safety filters (BAN / adult/sensitive)
# =============================================================================
BAN_WORDS = [
    # illegal / violence / hate (basic)
    "kill", "murder", "bomb", "weapon", "terrorist",
    # explicit adult
    "porn", "nude", "sex", "blowjob", "dick", "vagina",
    # self-harm
    "suicide", "self-harm",
]

BAN_WORDS_JA = [
    "ÊÆ∫", "ÁàÜÂºæ", "Ê≠¶Âô®", "„ÉÜ„É≠",
    "„Éù„É´„Éé", "Ë£∏", "ÊÄßË°åÁÇ∫", "„Å°„Çì„Åì", "„Åæ„Çì„Åì",
    "Ëá™ÊÆ∫", "Ëá™ÂÇ∑",
]

def adult_or_sensitive(text: str) -> bool:
    t = (text or "").lower()
    if any(w in t for w in BAN_WORDS):
        return True
    if any(w in (text or "") for w in BAN_WORDS_JA):
        return True
    return False


def too_broad_vent(text: str) -> bool:
    """
    Downrank content that is mainly venting with no actionable question.
    """
    t = (text or "").lower()
    # if there is no question-like marker and mostly abstract emotion words
    has_question = any(x in t for x in ["?", "how", "what", "which", "where", "when", "why", "help", "fix", "recommend", "best", "compare", "plan", "checklist"])
    emo = sum(1 for x in ["hate", "tired", "annoying", "frustrated", "sad", "depressed", "angry", "worst", "sucks"] if x in t)
    if (not has_question) and emo >= 2:
        return True
    return False


# =============================================================================
# Data models
# =============================================================================
@dataclass
class Post:
    source: str
    id: str
    url: str
    text: str
    author: str
    created_at: str
    lang_hint: str = ""
    meta: Optional[Dict[str, Any]] = None

    def norm_text(self) -> str:
        t = self.text or ""
        t = re.sub(r"\s+", " ", t).strip()
        return t


@dataclass
class Theme:
    title: str
    search_title: str
    slug: str
    category: str
    problem_list: List[str]
    representative_posts: List[Post]
    score: float
    keywords: List[str]
    short_code: str = ""  # /goliath/go/<code>/


# =============================================================================
# Collectors
# =============================================================================
KEYWORDS = [
    # tech
    "error", "issue", "help", "how do i", "how to", "can't", "cannot", "failed", "fix", "bug",
    "login", "password", "token", "oauth", "dns", "cname", "aaaa", "ssl", "github pages",
    "pdf", "convert", "compress", "mp4", "ffmpeg", "excel", "spreadsheet", "formula",
    # life / planning
    "itinerary", "travel plan", "packing list", "layover", "eSIM", "refund", "cancellation", "budget",
    "recipe", "meal prep", "calories", "protein", "sleep", "workout", "habit", "routine",
    "study plan", "memorize", "procrastination", "focus", "schedule", "checklist", "template",
    "resume", "interview", "anxiety", "compare", "recommend", "best",
    "move", "declutter", "cleaning", "laundry",
]

def collect_bluesky(max_items: int = 60) -> List[Post]:
    """
    ATProto:
      - createSession: https://bsky.social/xrpc/com.atproto.server.createSession
      - searchPosts:   https://bsky.social/xrpc/app.bsky.feed.searchPosts?q=...
    """
    if not (BLUESKY_HANDLE and BLUESKY_APP_PASSWORD):
        logging.info("Bluesky: skipped (missing BLUESKY_HANDLE/BLUESKY_APP_PASSWORD)")
        return []

    logging.info("Bluesky: collecting up to %d", max_items)
    status, js, raw = http_post_json(
        "https://bsky.social/xrpc/com.atproto.server.createSession",
        {"identifier": BLUESKY_HANDLE, "password": BLUESKY_APP_PASSWORD},
        headers={"Accept": "application/json"},
        timeout=20,
    )
    if status != 200 or "accessJwt" not in js:
        logging.warning("Bluesky: session failed status=%s body=%s", status, (raw or "")[:300])
        return []

    token = js["accessJwt"]
    headers = {"Authorization": f"Bearer {token}", "Accept": "application/json"}

    # Mix tech + life
    queries = [
        # tech
        "how to fix error",
        "can't login help",
        "pdf convert fails",
        "compress mp4 best settings",
        "excel formula wrong",
        "github pages custom domain dns",
        "oauth token expired",
        "privacy settings cookies",
        # life
        "itinerary planner help",
        "packing list checklist",
        "layover eSIM advice",
        "refund cancellation policy",
        "meal prep plan",
        "calories protein plan",
        "sleep schedule fix",
        "workout routine beginner",
        "study plan schedule",
        "procrastination can't focus",
        "resume interview help",
        "budget template",
        "compare best option",
        "weekend plan ideas",
    ]

    out: List[Post] = []
    for q in queries:
        if len(out) >= max_items:
            break
        url = "https://bsky.social/xrpc/app.bsky.feed.searchPosts?" + urlencode({"q": q, "limit": 25})
        st, body = http_get(url, headers=headers, timeout=20)
        if st != 200:
            continue
        try:
            data = json.loads(body)
        except Exception:
            continue

        for item in data.get("posts", []):
            if len(out) >= max_items:
                break
            uri = item.get("uri", "") or ""
            cid = item.get("cid", "") or ""
            record = item.get("record") or {}
            text = (record.get("text") or item.get("text") or "").strip()
            author = ((item.get("author") or {}).get("handle") or "unknown").strip()
            created_at = (record.get("createdAt") or item.get("indexedAt") or now_iso())

            if not text or adult_or_sensitive(text):
                continue

            post_url = ""
            if uri:
                try:
                    rkey = uri.split("/")[-1]
                    post_url = f"https://bsky.app/profile/{author}/post/{rkey}"
                except Exception:
                    post_url = uri

            if not post_url:
                continue

            pid = sha1(f"bsky:{uri}:{cid}:{post_url}")
            out.append(Post(
                source="bluesky",
                id=pid,
                url=post_url,
                text=text,
                author=author,
                created_at=created_at,
                meta={"query": q, "uri": uri, "cid": cid},
            ))

    logging.info("Bluesky: collected %d", len(out))
    return out


def collect_mastodon(max_items: int = 120) -> List[Post]:
    """
    Mastodon:
      - public timeline: /api/v1/timelines/public?limit=
      - tag timeline: /api/v1/timelines/tag/{tag}?limit=
      - search: /api/v2/search?q=...&type=statuses&resolve=true
    """
    if not (MASTODON_BASE and MASTODON_TOKEN):
        logging.info("Mastodon: skipped (missing MASTODON_BASE/MASTODON_TOKEN)")
        return []

    base = MASTODON_BASE.rstrip("/")
    headers = {"Authorization": f"Bearer {MASTODON_TOKEN}", "Accept": "application/json"}
    logging.info("Mastodon: collecting up to %d from %s", max_items, base)

    tags = [
        # tech
        "help", "support", "webdev", "privacy", "excel", "opensource", "github", "dns", "linux",
        "pdf", "ffmpeg",
        # life
        "travel", "itinerary", "packing", "cooking", "mealprep", "fitness", "sleep",
        "studytips", "productivity", "personalfinance", "career", "relationships",
    ]
    queries = [
        # tech + life triggers
        "need help", "how to fix", "error", "cannot", "failed", "issue", "bug",
        "itinerary", "packing list", "meal prep", "workout plan", "sleep schedule",
        "study plan", "resume", "interview", "budget template", "compare", "recommend",
    ]

    out: List[Post] = []

    def add_statuses(statuses: List[Dict[str, Any]], hint: str) -> None:
        nonlocal out
        for s in statuses:
            if len(out) >= max_items:
                return
            sid = s.get("id", "") or ""
            url = (s.get("url") or "").strip()
            created_at = (s.get("created_at") or now_iso())
            acct = ((s.get("account") or {}).get("acct") or "unknown").strip()
            content = (s.get("content") or "")
            text = re.sub(r"<[^>]+>", " ", content)
            text = html.unescape(text).strip()

            if not url or not text or adult_or_sensitive(text):
                continue

            pid = sha1(f"mstdn:{sid}:{url}")
            out.append(Post(
                source="mastodon",
                id=pid,
                url=url,
                text=text,
                author=acct,
                created_at=created_at,
                meta={"hint": hint},
            ))

    st, body = http_get(f"{base}/api/v1/timelines/public?limit=40", headers=headers, timeout=20)
    if st == 200:
        try:
            add_statuses(json.loads(body), "public")
        except Exception:
            pass

    for tag in tags:
        if len(out) >= max_items:
            break
        st, body = http_get(f"{base}/api/v1/timelines/tag/{quote(tag)}?limit=30", headers=headers, timeout=20)
        if st != 200:
            continue
        try:
            add_statuses(json.loads(body), f"tag:{tag}")
        except Exception:
            continue

    for q in queries:
        if len(out) >= max_items:
            break
        url = f"{base}/api/v2/search?" + urlencode({"q": q, "type": "statuses", "resolve": "true", "limit": "20"})
        st, body = http_get(url, headers=headers, timeout=20)
        if st != 200:
            continue
        try:
            data = json.loads(body)
            add_statuses(data.get("statuses", []) or [], f"search:{q}")
        except Exception:
            continue

    logging.info("Mastodon: collected %d", len(out))
    return out


def reddit_oauth_token() -> Optional[str]:
    if not (REDDIT_CLIENT_ID and REDDIT_CLIENT_SECRET and REDDIT_REFRESH_TOKEN):
        return None

    token_url = "https://www.reddit.com/api/v1/access_token"
    basic = "Basic " + base64_basic_auth(REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET)
    headers = {
        "Authorization": basic,
        "User-Agent": REDDIT_USER_AGENT,
        "Content-Type": "application/x-www-form-urlencoded",
        "Accept": "application/json",
    }
    form = urlencode({"grant_type": "refresh_token", "refresh_token": REDDIT_REFRESH_TOKEN}).encode("utf-8")
    req = Request(token_url, headers=headers, data=form, method="POST")
    try:
        with urlopen(req, timeout=20) as resp:
            raw = resp.read().decode("utf-8", errors="replace")
            js = json.loads(raw)
            return js.get("access_token")
    except Exception as e:
        logging.warning("Reddit: oauth token failed: %s", str(e))
        return None


def collect_reddit(max_items: int = 60) -> List[Post]:
    """
    Reddit:
      - If OAuth creds exist: use https://oauth.reddit.com
      - Else: use public JSON endpoints (rate-limited)
    """
    subs = [x.strip() for x in (REDDIT_SUBREDDITS or "").split(",") if x.strip()]
    if not subs:
        subs = ["webdev", "sysadmin", "programming"]

    token = reddit_oauth_token()
    if token:
        base = "https://oauth.reddit.com"
        headers = {"Authorization": f"bearer {token}", "User-Agent": REDDIT_USER_AGENT, "Accept": "application/json"}
        logging.info("Reddit: OAuth mode collecting up to %d", max_items)
    else:
        base = "https://www.reddit.com"
        headers = {"User-Agent": REDDIT_USER_AGENT, "Accept": "application/json"}
        logging.info("Reddit: public mode collecting up to %d", max_items)

    triggers = [k.lower() for k in KEYWORDS]
    out: List[Post] = []

    for sub in subs:
        if len(out) >= max_items:
            break

        st, body = http_get(f"{base}/r/{quote(sub)}/new.json?limit=50", headers=headers, timeout=20)
        if st != 200:
            continue

        try:
            data = json.loads(body)
        except Exception:
            continue

        children = (((data or {}).get("data") or {}).get("children") or [])
        for ch in children:
            if len(out) >= max_items:
                break
            d = (ch or {}).get("data") or {}
            title = (d.get("title") or "").strip()
            selftext = (d.get("selftext") or "").strip()
            text = (title + "\n" + selftext).strip()
            if not text or adult_or_sensitive(text):
                continue

            low = text.lower()
            if not any(t in low for t in triggers):
                continue

            permalink = (d.get("permalink") or "").strip()
            url = ("https://www.reddit.com" + permalink) if permalink.startswith("/") else ((d.get("url") or "").strip())
            if not url:
                continue

            author = (d.get("author") or "unknown").strip()
            created_utc = d.get("created_utc") or time.time()
            created_at = dt.datetime.fromtimestamp(float(created_utc), tz=dt.timezone.utc).astimezone().isoformat(timespec="seconds")
            rid = d.get("name") or d.get("id") or sha1(url)

            pid = sha1(f"reddit:{rid}:{url}")
            out.append(Post(
                source="reddit",
                id=pid,
                url=url,
                text=text,
                author=author,
                created_at=created_at,
                meta={"subreddit": sub},
            ))

    logging.info("Reddit: collected %d", len(out))
    return out


def collect_hn(max_items: int = 70) -> List[Post]:
    """
    Hacker News (Algolia search_by_date for help-like content)
    """
    max_items = clamp(max_items, 10, 200)
    url = "https://hn.algolia.com/api/v1/search_by_date?" + urlencode({
        "query": HN_QUERY,
        "tags": "story,comment",
        "hitsPerPage": str(min(max_items, 100)),
        "page": "0",
    })
    st, body = http_get(url, headers={"Accept": "application/json"}, timeout=20)
    if st != 200:
        logging.warning("HN: failed status=%s body=%s", st, (body or "")[:200])
        return []

    try:
        data = json.loads(body)
    except Exception:
        return []

    hits = data.get("hits", []) or []
    out: List[Post] = []
    for h in hits:
        if len(out) >= max_items:
            break
        text = (h.get("title") or "") + "\n" + (h.get("comment_text") or "")
        text = re.sub(r"<[^>]+>", " ", text)
        text = html.unescape(text).strip()
        if not text or adult_or_sensitive(text):
            continue

        object_id = h.get("objectID") or ""
        created_at = h.get("created_at") or now_iso()
        author = h.get("author") or "unknown"

        hn_url = h.get("url") or ""
        if not hn_url:
            hn_url = f"https://news.ycombinator.com/item?id={object_id}"

        pid = sha1(f"hn:{object_id}:{hn_url}")
        out.append(Post(
            source="hn",
            id=pid,
            url=hn_url,
            text=text,
            author=author,
            created_at=created_at,
            meta={"points": h.get("points", 0), "tags": h.get("_tags", [])},
        ))

    logging.info("HN: collected %d", len(out))
    return out


# =============================================================================
# X state (duplicate prevention)
# =============================================================================


def load_last_seen() -> Dict[str, Any]:
    d = read_json(LAST_SEEN_PATH, default={})
    if not isinstance(d, dict):
        d = {}
    if "x_seen" not in d or not isinstance(d.get("x_seen"), list):
        d["x_seen"] = []
    return d

def save_last_seen(d: Dict[str, Any]) -> None:
    os.makedirs(STATE_DIR, exist_ok=True)
    # keep small (latest 200 ids)
    seen = d.get("x_seen") or []
    if isinstance(seen, list):
        d["x_seen"] = seen[-200:]
    write_json(LAST_SEEN_PATH, d)

def collect_x_mentions(max_items: int = 1) -> List[Post]:
    """
    X v2: Keyword Search -> pick 1 tweet -> avoid duplicates via state/last_seen.json
    - 1Âõû„ÅÆÂÆüË°å„Åß„ÄåÊé°Áî®„ÅØ1‰ª∂„Äç„Å´Âõ∫ÂÆöÔºàreadÁØÄÁ¥ÑÔºâ
    - Ê§úÁ¥¢„Å†„Åë„ÅßÊú¨Êñá(text)„ÅØÂèñ„Çå„Çã„ÅÆ„Åß„ÄÅËøΩÂä†„ÅÆtweetÂèñÂæó„ÅØ„Åó„Å™„ÅÑÔºà= ÂÆüË≥™1„É™„ÇØ„Ç®„Çπ„ÉàÂâçÊèêÔºâ
    """
    if not X_BEARER_TOKEN:
        logging.info("X: skipped (missing X_BEARER_TOKEN or aliases)")
        return []

    max_items = 1  # Âº∑Âà∂Ôºö1‰ª∂„Å†„ÅëÊé°Áî®
    headers = {"Authorization": f"Bearer {X_BEARER_TOKEN}", "Accept": "application/json"}

    # „ÇØ„Ç®„É™ÔºàÊú™ÊåáÂÆö„Å™„ÇâÁúÅ„Ç®„Éç„ÅÆÂõ∫ÂÆö„ÇØ„Ç®„É™Ôºâ
    q = (X_QUERY or '("how to" OR help OR error OR failed OR bug OR fix) -is:retweet -is:reply').strip()

    # recent search
    url = f"{X_API_BASE.rstrip('/')}/2/tweets/search/recent?" + urlencode({
        "query": q,
        "max_results": "10",
        "tweet.fields": "created_at,lang,author_id",
    })

    st, body = http_get(url, headers=headers, timeout=20)
    if st != 200:
        logging.warning("X: search failed status=%s body=%s", st, (body or "")[:200])
        return []

    try:
        data = json.loads(body)
    except Exception:
        return []

    tweets = data.get("data") or []
    if not tweets:
        logging.info("X: collected 0")
        return []

    state = load_last_seen()
    seen = set(state.get("x_seen") or [])

    picked = None
    for t in tweets:
        tid = (t.get("id") or "").strip()
        if not tid:
            continue
        if tid in seen:
            continue
        picked = t
        break

    if not picked:
        logging.info("X: collected 0 (all duplicates)")
        return []

    tid = picked.get("id") or ""
    text = (picked.get("text") or "").strip()
    if not text or adult_or_sensitive(text):
        logging.info("X: collected 0 (filtered)")
        return []

    created_at = picked.get("created_at") or now_iso()
    author = picked.get("author_id") or "unknown"
    post_url = f"https://x.com/i/web/status/{tid}"
    pid = sha1(f"x:{tid}:{post_url}")

    # save state (commit will persist)
    state["x_seen"] = (state.get("x_seen") or []) + [tid]
    save_last_seen(state)

    out = [Post(
        source="x",
        id=pid,
        url=post_url,
        text=text,
        author=str(author),
        created_at=created_at,
        lang_hint=picked.get("lang") or "",
        meta={"query": q, "author_id": author},
    )]

    logging.info("X: collected %d (picked 1)", len(out))
    return out



# =============================================================================
# Normalization & Clustering
# =============================================================================
STOPWORDS_EN = set("""
a an the and or but if then else when while of for to in on at from by with without into onto over under
is are was were be been being do does did done have has had will would can could should may might
this that these those it its i'm youre you're we they them our your my mine me you he she his her
""".split())

STOPWORDS_JA = set(["„Åì„Çå", "„Åù„Çå", "„ÅÇ„Çå", "„Åü„ÇÅ", "„ÅÆ„Åß", "„Åã„Çâ", "„Åß„Åô", "„Åæ„Åô", "„ÅÑ„Çã", "„ÅÇ„Çã", "„Å™„Çã", "„Åì„Å®", "„ÇÇ„ÅÆ", "„Çà„ÅÜ", "„Å∏", "„Å´", "„Çí", "„Åå", "„Å®", "„Åß", "„ÇÇ"])

def simple_tokenize(text: str) -> List[str]:
    t = (text or "").lower()
    t = re.sub(r"https?://\S+", " ", t)
    t = re.sub(r"[\[\]()<>{}‚Äª*\"'`~^|\\]", " ", t)
    t = re.sub(r"[^0-9a-z\u3040-\u30ff\u4e00-\u9fff\s\-_/.:]", " ", t)
    t = re.sub(r"\s+", " ", t).strip()

    parts: List[str] = []
    for p in t.split():
        if len(p) <= 1:
            continue
        if p in STOPWORDS_EN:
            continue
        if p in STOPWORDS_JA:
            continue
        parts.append(p)

    # crude JP chunks to help clustering without full tokenizer
    jp_chunks = re.findall(r"[\u3040-\u30ff\u4e00-\u9fff]{2,}", t)
    parts.extend([c for c in jp_chunks if c not in STOPWORDS_JA and len(c) >= 2])

    return parts[:100]


def jaccard(a: set, b: set) -> float:
    if not a or not b:
        return 0.0
    inter = len(a & b)
    union = len(a | b)
    return inter / union if union else 0.0


def cluster_posts(posts: List[Post], threshold: float = 0.22) -> List[List[Post]]:
    """
    Lightweight clustering by Jaccard similarity of token sets.
    """
    logging.info("Clustering %d posts (threshold=%.2f)", len(posts), threshold)
    token_sets: Dict[str, set] = {p.id: set(simple_tokenize(p.norm_text())) for p in posts}

    clusters: List[List[Post]] = []
    used = set()

    for i, p in enumerate(posts):
        if p.id in used:
            continue
        used.add(p.id)
        base = token_sets[p.id]
        c = [p]
        for q in posts[i + 1:]:
            if q.id in used:
                continue
            sim = jaccard(base, token_sets[q.id])
            if sim >= threshold:
                used.add(q.id)
                c.append(q)
        clusters.append(c)

    clusters.sort(key=lambda x: (-len(x), x[0].created_at))
    logging.info("Clusters: %d (top sizes=%s)", len(clusters), [len(c) for c in clusters[:8]])
    return clusters


def extract_keywords(posts: List[Post], topk: int = 14) -> List[str]:
    freq: Dict[str, int] = {}
    for p in posts:
        for w in simple_tokenize(p.norm_text()):
            freq[w] = freq.get(w, 0) + 1
    items = sorted(freq.items(), key=lambda kv: (-kv[1], kv[0]))
    return [k for k, _ in items[:topk]]


def choose_category(posts: List[Post], keywords: List[str]) -> str:
    """
    Heuristic category selection across fixed 22 categories.
    """
    text = " ".join([p.norm_text() for p in posts]).lower()
    k = set([x.lower() for x in keywords])

    def has_any(words: List[str]) -> bool:
        return any(w in text for w in words) or any(w in k for w in words)

    # tech
    if has_any(["dns", "cname", "aaaa", "a record", "nameserver", "github pages", "hosting", "ssl", "https"]):
        return "Web/Hosting"
    if has_any(["python", "node", "npm", "pip", "powershell", "bash", "cli", "library", "compile", "stack", "trace", "dev"]):
        return "Dev/Tools"
    if has_any(["automation", "workflow", "cron", "github actions", "llm", "openai", "prompt", "agent"]):
        return "AI/Automation"
    if has_any(["privacy", "security", "2fa", "phishing", "cookie", "vpn", "encryption", "leak"]):
        return "Security/Privacy"
    if has_any(["video", "mp4", "compress", "codec", "ffmpeg", "audio", "subtitle"]):
        return "Media"
    if has_any(["pdf", "docx", "ppt", "docs", "word", "convert", "merge", "compress pdf"]):
        return "PDF/Docs"
    if has_any(["image", "png", "jpg", "webp", "design", "figma", "photoshop", "illustrator"]):
        return "Images/Design"
    if has_any(["excel", "spreadsheet", "csv", "google sheets", "vlookup", "pivot", "formula"]):
        return "Data/Spreadsheets"
    if has_any(["invoice", "tax", "accounting", "bookkeeping", "receipt", "vat"]):
        return "Business/Accounting/Tax"
    if has_any(["seo", "marketing", "ads", "social", "instagram", "tiktok", "youtube", "growth"]):
        return "Marketing/Social"
    if has_any(["productivity", "todo", "note", "calendar", "time management", "procrastination", "focus"]):
        return "Productivity"
    if has_any(["english", "language", "toeic", "eiken", "ielts"]):
        return "Education/Language"

    # life
    if has_any(["travel", "trip", "hotel", "itinerary", "flight", "booking", "layover", "packing", "esim"]):
        return "Travel/Planning"
    if has_any(["recipe", "cook", "cooking", "meal prep", "kitchen", "grocery"]):
        return "Food/Cooking"
    if has_any(["workout", "fitness", "diet", "health", "running", "sleep", "calories", "protein"]):
        return "Health/Fitness"
    if has_any(["study", "learning", "exam", "homework", "memorize", "flashcards"]):
        return "Study/Learning"
    if has_any(["money", "budget", "loan", "invest", "stock", "fees", "refund"]):
        return "Money/Personal Finance"
    if has_any(["career", "job", "resume", "cv", "interview", "apply"]):
        return "Career/Work"
    if has_any(["relationship", "communication", "friend", "chat", "texting", "awkward"]):
        return "Relationships/Communication"
    if has_any(["home", "rent", "utility", "life admin", "paperwork", "moving", "declutter", "cleaning"]):
        return "Home/Life Admin"
    if has_any(["buy", "shopping", "product", "recommend", "compare", "best", "value"]):
        return "Shopping/Products"
    if has_any(["event", "ticket", "concert", "sports", "weekend plan", "date plan", "rainy day"]):
        return "Events/Leisure"

    return "Dev/Tools"


def score_cluster(posts: List[Post], category: str) -> float:
    """
    Score: cluster size + solvable tool signal + life ‚Äúdecision urgency‚Äù signals.
    """
    size = len(posts)
    text = " ".join([p.norm_text() for p in posts]).lower()

    solvable_signals = [
        "how", "fix", "error", "failed", "can't", "cannot", "help",
        "Ë®≠ÂÆö", "Áõ¥„Åó", "ÂéüÂõ†", "„Ç®„É©„Éº", "„Åß„Åç„Å™„ÅÑ", "‰∏çÂÖ∑Âêà", "Â§±Êïó",
    ]
    tool_signals = [
        "convert", "compress", "calculator", "generator", "planner", "template", "checklist", "step-by-step", "schedule",
        "Â§âÊèõ", "ÂúßÁ∏Æ", "Ë®àÁÆó", "„ÉÅ„Çß„ÉÉ„ÇØ", "„ÉÜ„É≥„Éó„É¨", "„ÉÑ„Éº„É´", "ÊâãÈ†Ü",
    ]
    life_decision = [
        "plan", "itinerary", "packing", "what should i do", "recommend", "best", "compare", "budget", "schedule",
        "checklist", "template", "step by step", "meal prep", "study plan",
    ]
    urgency = [
        "urgent", "today", "tomorrow", "this week", "before i go", "deadline", "soon", "asap",
        "‰ªäÊó•", "ÊòéÊó•", "‰ªäÈÄ±", "Âá∫Áô∫Ââç", "Á∑†Âàá",
    ]
    stuck = [
        "i'm stuck", "confused", "overwhelmed", "don't know what to choose", "not sure", "anxiety",
        "Ë©∞„Çì„Å†", "„Çè„Åã„Çâ„Å™„ÅÑ", "Ëø∑„ÅÜ", "‰∏çÂÆâ",
    ]

    s1 = sum(1 for w in solvable_signals if w in text)
    s2 = sum(1 for w in tool_signals if w in text)
    s3 = sum(1 for w in life_decision if w in text)
    s4 = sum(1 for w in urgency if w in text)
    s5 = sum(1 for w in stuck if w in text)

    score = size * 1.8 + s1 * 0.5 + s2 * 0.7 + s3 * 0.55 + s4 * 0.45 + s5 * 0.35

    if too_broad_vent(text):
        score *= 0.75

    # mild balancing so life categories can compete
    if category in ["Travel/Planning", "Food/Cooking", "Health/Fitness", "Study/Learning", "Money/Personal Finance",
                    "Career/Work", "Relationships/Communication", "Home/Life Admin", "Shopping/Products", "Events/Leisure"]:
        score *= 1.12

    return float(score)


def build_search_title(category: str, keywords: List[str]) -> str:
    """
    Force titles toward ‚Äúsearch query‚Äù style (EN) + includes tool-ish noun.
    """
    kw = [k for k in keywords if len(k) <= 18][:6]
    base = " ".join(kw[:3]).strip()
    if not base:
        base = category.replace("/", " ")

    if category == "Travel/Planning":
        return f"{base} itinerary planner checklist"
    if category == "Food/Cooking":
        return f"{base} meal prep plan + shopping list"
    if category == "Health/Fitness":
        return f"{base} workout plan + habit tracker"
    if category == "Study/Learning":
        return f"{base} study plan schedule template"
    if category == "Money/Personal Finance":
        return f"{base} budget planner + fee checklist"
    if category == "Career/Work":
        return f"{base} resume checklist + interview prep"
    if category == "Relationships/Communication":
        return f"{base} conversation templates + awkwardness fixes"
    if category == "Home/Life Admin":
        return f"{base} moving checklist + life admin planner"
    if category == "Shopping/Products":
        return f"{base} compare tool + buying checklist"
    if category == "Events/Leisure":
        return f"{base} weekend plan generator + checklist"
    # tech
    if category == "Web/Hosting":
        return f"{base} DNS/SSL fix checklist"
    if category == "PDF/Docs":
        return f"{base} PDF convert/merge checklist"
    if category == "Media":
        return f"{base} video compression settings checklist"
    if category == "Data/Spreadsheets":
        return f"{base} spreadsheet formula fix checklist"
    if category == "Security/Privacy":
        return f"{base} privacy settings + login fix checklist"
    if category == "AI/Automation":
        return f"{base} automation workflow fix checklist"
    if category == "Marketing/Social":
        return f"{base} social growth checklist + template"
    if category == "Education/Language":
        return f"{base} language study plan template"
    return f"{base} fix guide checklist tool"


def make_theme(posts: List[Post]) -> Theme:
    keywords = extract_keywords(posts)
    category = choose_category(posts, keywords)
    score = score_cluster(posts, category)

    search_title = build_search_title(category, keywords)
    base_slug = safe_slug(search_title)
    # collision-safe slug allocation happens later
    title = f"{search_title} | {SITE_BRAND}"

    problems: List[str] = []
    for p in posts[:12]:
        line = p.norm_text()[:140].rstrip()
        if line:
            problems.append(line)
    problems = uniq_keep_order([re.sub(r"\s+", " ", x) for x in problems])

    while len(problems) < 10:
        problems.append(f"Trouble related to {category}: symptom #{len(problems)+1}")
    problems = problems[:20]

    return Theme(
        title=title,
        search_title=search_title,
        slug=base_slug,  # may be adjusted with -2,-3 on collision
        category=category,
        problem_list=problems,
        representative_posts=posts[: min(len(posts), 8)],
        score=score,
        keywords=keywords,
        short_code="",
    )


# =============================================================================
# Affiliates
# =============================================================================
def load_affiliates() -> Dict[str, Any]:
    data = read_json(AFFILIATES_JSON, default={})
    if not isinstance(data, dict):
        return {}
    return data


def normalize_affiliates_shape(aff: Dict[str, Any]) -> Dict[str, List[Dict[str, Any]]]:
    """
    Accept:
      - { "categories": { "<CAT>": [..], ... } }
      - or { "<CAT>": [..], ... }
    Return { "<CAT>": [ dict, ... ] } only for keys in CATEGORIES_22.
    """
    categories: Dict[str, Any] = {}
    if isinstance(aff.get("categories"), dict):
        categories = aff["categories"]
    else:
        categories = aff

    out: Dict[str, List[Dict[str, Any]]] = {}
    for cat in CATEGORIES_22:
        v = categories.get(cat, [])
        if isinstance(v, list):
            out[cat] = [x for x in v if isinstance(x, dict)]
        else:
            out[cat] = []
    return out


def sanitize_affiliate_html(h: str) -> str:
    """
    Script tags forbidden. Keep existing approach: strip <script ...>...</script>.
    """
    if not h:
        return ""
    h2 = re.sub(r"(?is)<script[^>]*>.*?</script>", "", h)
    return h2.strip()


def pick_affiliates_for_category(aff_norm: Dict[str, List[Dict[str, Any]]], category: str, topn: int = 2) -> List[Dict[str, Any]]:
    items = aff_norm.get(category, []) or []

    def pr(x: Dict[str, Any]) -> float:
        try:
            return float(x.get("priority", 0))
        except Exception:
            return 0.0

    cleaned: List[Dict[str, Any]] = []
    for x in items:
        html_code = x.get("html", "") or ""
        if html_code:
            x2 = dict(x)
            x2["html"] = sanitize_affiliate_html(str(html_code))
            cleaned.append(x2)
        elif x.get("url"):
            cleaned.append(x)
        else:
            # ignore items without html/url
            pass

    cleaned.sort(key=lambda x: -pr(x))
    return cleaned[:topn]


def audit_affiliate_keys(aff_raw: Dict[str, Any]) -> Dict[str, Any]:
    """
    Check affiliates.json keys match GENRES (CATEGORIES_22). Missing keys => issue note.
    """
    if isinstance(aff_raw.get("categories"), dict):
        keys = set(aff_raw["categories"].keys())
    elif isinstance(aff_raw, dict):
        keys = set(aff_raw.keys())
    else:
        keys = set()

    # ignore "categories" wrapper key itself
    keys.discard("categories")

    missing = [c for c in CATEGORIES_22 if c not in keys]
    extra = sorted([k for k in keys if k not in set(CATEGORIES_22)])

    return {
        "missing": missing,
        "extra": extra,
        "ok": (len(missing) == 0),
    }


# =============================================================================
# Hub inventory (hub/sites.json) & routing features (categories / popular / new / purpose)
# ---- Affiliates: always define aff_norm (and aff_audit) BEFORE any use ----
def init_affiliates() -> tuple[dict[str, list[dict[str, Any]]], dict[str, Any]]:
    """
    Safe initializer for affiliates.
    Returns:
      - aff_norm: { "<CAT>": [ {..}, .. ] }
      - aff_audit: { missing: [...], extra: [...], ok: bool }
    """
    try:
        _aff_raw = load_affiliates()
        _audit = audit_affiliate_keys(_aff_raw)
        _norm = normalize_affiliates_shape(_aff_raw)
        return _norm, _audit
    except Exception:
        # ultra-safe fallback (no affiliates)
        _audit = {"missing": CATEGORIES_22[:], "extra": [], "ok": False}
        _norm = {c: [] for c in CATEGORIES_22}
        return _norm, _audit

aff_norm, aff_audit = init_affiliates()


# =============================================================================
def read_hub_sites() -> List[Dict[str, Any]]:
    data = read_json(HUB_SITES_JSON, default={})
    if isinstance(data, list):
        return [x for x in data if isinstance(x, dict)]
    if isinstance(data, dict) and isinstance(data.get("sites"), list):
        return [x for x in data["sites"] if isinstance(x, dict)]
    return []


def write_hub_sites(sites: List[Dict[str, Any]], aggregates: Dict[str, Any]) -> None:
    """
    hub frozen: ONLY update sites.json.
    (Do not touch hub/index.html or hub/assets.)
    """
    if is_frozen_path(HUB_SITES_JSON):
        # sites.json itself is allowed (not in frozen list). Still, keep safe.
        pass

    os.makedirs(HUB_DIR, exist_ok=True)
    payload = {
        "sites": sites,
        "aggregates": aggregates,  # categories / popular / new / purpose
        "updated_at": now_iso(),
    }
    write_json(HUB_SITES_JSON, payload)


def compute_aggregates(all_sites: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Provide hub-strengthening data via sites.json (categories list + popular/new/purpose routes).
    Even if hub frontend ignores it today, the data is ready and future-proof.
    """
    # categories
    cats: Dict[str, List[Dict[str, Any]]] = {}
    for cat in CATEGORIES_22:
        cats[cat] = []

    for s in all_sites:
        cat = s.get("category") or ""
        if cat in cats:
            cats[cat].append({
                "title": s.get("search_title") or s.get("title") or "Tool",
                "url": s.get("url") or "#",
                "slug": s.get("slug") or "",
            })

    for cat in cats:
        # stable ordering: title
        cats[cat].sort(key=lambda x: (x.get("title") or "").lower())

    # new: by updated_at / created_at
    def ts(s: Dict[str, Any]) -> float:
        iso = s.get("updated_at") or s.get("created_at") or ""
        try:
            return dt.datetime.fromisoformat(iso.replace("Z", "+00:00")).timestamp()
        except Exception:
            return 0.0

    new_sites = sorted(all_sites, key=ts, reverse=True)[:12]
    new_list = [{"title": s.get("search_title") or s.get("title") or "Tool", "url": s.get("url") or "#", "slug": s.get("slug") or ""} for s in new_sites]

    # popular: prefer views/score/popularity if present; else fallback to recency
    def pop_metric(s: Dict[str, Any]) -> float:
        for k in ["views", "score", "popularity"]:
            if k in s:
                try:
                    return float(s.get(k, 0))
                except Exception:
                    pass
        return ts(s)

    popular_sites = sorted(all_sites, key=pop_metric, reverse=True)[:12]
    popular_list = [{"title": s.get("search_title") or s.get("title") or "Tool", "url": s.get("url") or "#", "slug": s.get("slug") or ""} for s in popular_sites]

    # purpose routes: simple buckets for internal navigation
    purpose_buckets = {
        "Popular tools": popular_list[:8],
        "New tools": new_list[:8],
        "By purpose": [],  # filled below
    }

    purpose_keywords = {
        "convert": ["convert", "Â§âÊèõ", "pdf", "docx", "png", "mp4"],
        "time": ["time", "schedule", "calendar", "deadline", "study plan", "itinerary"],
        "productivity": ["template", "checklist", "planner", "workflow", "habit"],
        "pricing": ["budget", "fees", "cost", "price", "compare", "refund"],
    }

    by_purpose: Dict[str, List[Dict[str, Any]]] = {k: [] for k in purpose_keywords.keys()}
    for s in all_sites:
        title = (s.get("search_title") or s.get("title") or "").lower()
        for bucket, words in purpose_keywords.items():
            if any(w.lower() in title for w in words):
                by_purpose[bucket].append({
                    "title": s.get("search_title") or s.get("title") or "Tool",
                    "url": s.get("url") or "#",
                    "slug": s.get("slug") or "",
                })

    # keep small
    for bucket in by_purpose:
        by_purpose[bucket] = by_purpose[bucket][:12]

    purpose_buckets["By purpose"] = [{"bucket": k, "items": v} for k, v in by_purpose.items()]

    return {
        "categories": cats,
        "popular": popular_list,
        "new": new_list,
        "purpose": purpose_buckets,
    }


# =============================================================================
# Shortlinks (for ‚Äúshort URL + one-line value‚Äù)
# =============================================================================
BASE62 = "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"

def to_base62(n: int) -> str:
    if n == 0:
        return "0"
    out = []
    while n > 0:
        n, r = divmod(n, 62)
        out.append(BASE62[r])
    return "".join(reversed(out))

def short_code_for_url(url: str) -> str:
    h = hashlib.sha1(url.encode("utf-8")).hexdigest()[:10]
    n = int(h, 16)
    code = to_base62(n)
    return code[:8]

def build_shortlink_page(target_url: str, code: str) -> Tuple[str, str]:
    """
    Returns (relative_path_under_repo, html_content)
    Short link lives under: goliath/go/<code>/index.html
    """
    rel_dir = os.path.join("goliath", "go", code)
    rel_path = os.path.join(rel_dir, "index.html")
    esc = html.escape(target_url, quote=True)
    content = f"""<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="robots" content="noindex">
  <meta http-equiv="refresh" content="0;url={esc}">
  <link rel="canonical" href="{esc}">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Redirect</title>
</head>
<body style="font-family:system-ui,Segoe UI,Roboto,Arial,sans-serif;padding:24px;">
  <p>Redirecting‚Ä¶</p>
  <p><a href="{esc}">{esc}</a></p>
  <script>location.replace("{esc}");</script>
</body>
</html>
"""
    return rel_path, content


# =============================================================================
# i18n dictionaries (core UI strings)
# =============================================================================
I18N = {'en': {'home': 'Home',
        'about': 'About Us',
        'all_tools': 'All Tools',
        'language': 'Language',
        'share': 'Share',
        'problems': 'Problems this tool can help with',
        'tool': 'Tool',
        'quick_answer': 'Quick answer',
        'causes': 'Common causes',
        'steps': 'Step-by-step checklist',
        'pitfalls': 'Common pitfalls & how to avoid them',
        'next': 'If it still doesn‚Äôt work',
        'faq': 'FAQ',
        'references': 'Reference links',
        'supplement': 'Supplementary resources',
        'related': 'Related tools',
        'popular': 'Popular tools',
        'disclaimer': 'Disclaimer',
        'terms': 'Terms',
        'privacy': 'Privacy',
        'contact': 'Contact',
        'footer_note': 'Practical, fast, and respectful guides‚Äîbuilt to reduce wasted trial-and-error.',
        'aff_title': 'Recommended',
        'copy': 'Copy',
        'copied': 'Copied',
        'short_value': 'Do it in 3 seconds',
        'tool_input': 'Input',
        'tool_input_hint': '(paste your details)',
        'tool_placeholder': 'Example: dates, constraints, what you tried, what you need‚Ä¶',
        'tool_generate': 'Generate',
        'tool_clear': 'Clear',
        'tool_tip': 'Tip: include the exact error message and what changed recently.'},
 'ja': {'home': 'Home',
        'about': 'About Us',
        'all_tools': 'All Tools',
        'language': 'Ë®ÄË™û',
        'share': 'ÂÖ±Êúâ',
        'problems': '„Åì„ÅÆ„ÉÑ„Éº„É´„ÅåÂä©„Åë„ÇãÊÇ©„Åø‰∏ÄË¶ß',
        'tool': '„ÉÑ„Éº„É´',
        'quick_answer': 'ÁµêË´ñÔºàÊúÄÁü≠„ÅßÁõ¥„ÅôÊñπÈáùÔºâ',
        'causes': 'ÂéüÂõ†„ÅÆ„Éë„Çø„Éº„É≥ÂàÜ„Åë',
        'steps': 'ÊâãÈ†ÜÔºà„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„ÉàÔºâ',
        'pitfalls': '„Çà„Åè„ÅÇ„ÇãÂ§±Êïó„Å®ÂõûÈÅøÁ≠ñ',
        'next': 'Áõ¥„Çâ„Å™„ÅÑÂ†¥Âêà„ÅÆÊ¨°„ÅÆÊâã',
        'faq': 'FAQ',
        'references': 'ÂèÇËÄÉURL',
        'supplement': 'Ë£úÂä©Ë≥áÊñô',
        'related': 'Èñ¢ÈÄ£„ÉÑ„Éº„É´',
        'popular': '‰∫∫Ê∞ó„ÅÆ„ÉÑ„Éº„É´',
        'disclaimer': 'ÂÖçË≤¨‰∫ãÈ†Ö',
        'terms': 'Âà©Áî®Ë¶èÁ¥Ñ',
        'privacy': '„Éó„É©„Ç§„Éê„Ç∑„Éº„Éù„É™„Ç∑„Éº',
        'contact': '„ÅäÂïè„ÅÑÂêà„Çè„Åõ',
        'footer_note': 'ÂÆüÂãô„Åß‰Ωø„Åà„ÇãÊâãÈ†Ü„Å´ÂØÑ„Åõ„Å¶„ÄÅÁü≠ÊôÇÈñì„ÅßËß£Ê±∫„Åß„Åç„ÇãÂΩ¢„ÇíÁõÆÊåá„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ',
        'aff_title': '„Åä„Åô„Åô„ÇÅ',
        'copy': '„Ç≥„Éî„Éº',
        'copied': '„Ç≥„Éî„Éº„Åó„Åæ„Åó„Åü',
        'short_value': '3Áßí„Åß„Åß„Åç„Çã',
        'tool_input': 'ÂÖ•Âäõ',
        'tool_input_hint': 'ÔºàÁä∂Ê≥Å„ÇíË≤º„Çä‰ªò„ÅëÔºâ',
        'tool_placeholder': '‰æãÔºöÊó•ÊôÇ„ÄÅÂà∂Á¥Ñ„ÄÅË©¶„Åó„Åü„Åì„Å®„ÄÅÂøÖË¶Å„Å™„Åì„Å®‚Ä¶',
        'tool_generate': 'ÁîüÊàê',
        'tool_clear': '„ÇØ„É™„Ç¢',
        'tool_tip': '„Ç≥„ÉÑÔºö„Ç®„É©„Éº„É°„ÉÉ„Çª„Éº„Ç∏ÂÖ®Êñá„Å®„ÄåÊúÄËøëÂ§â„Åà„Åü„Åì„Å®„Äç„ÇíÂÖ•„Çå„Çã„Å®Á≤æÂ∫¶„Åå‰∏ä„Åå„Çä„Åæ„Åô„ÄÇ'},
 'ko': {'home': 'Home',
        'about': 'About Us',
        'all_tools': 'All Tools',
        'language': 'Ïñ∏Ïñ¥',
        'share': 'Í≥µÏú†',
        'problems': 'Ïù¥ ÎèÑÍµ¨Í∞Ä Ìï¥Í≤∞Ìï† Ïàò ÏûàÎäî Í≥†ÎØº',
        'tool': 'ÎèÑÍµ¨',
        'quick_answer': 'Í≤∞Î°†(Í∞ÄÏû• Îπ†Î•∏ Ìï¥Í≤∞ Î∞©Ìñ•)',
        'causes': 'ÏõêÏù∏ Ìå®ÌÑ¥',
        'steps': 'Ï≤¥ÌÅ¨Î¶¨Ïä§Ìä∏(Îã®Í≥ÑÎ≥Ñ)',
        'pitfalls': 'ÏûêÏ£º ÌïòÎäî Ïã§ÏàòÏôÄ ÌöåÌîºÎ≤ï',
        'next': 'Í≥ÑÏÜç Ïïà Îê† Îïå',
        'faq': 'FAQ',
        'references': 'Ï∞∏Í≥† ÎßÅÌÅ¨',
        'supplement': 'Ï∂îÍ∞Ä ÏûêÎ£å',
        'related': 'Í¥ÄÎ†® ÎèÑÍµ¨',
        'popular': 'Ïù∏Í∏∞ ÎèÑÍµ¨',
        'disclaimer': 'Î©¥Ï±Ö',
        'terms': 'Ïù¥Ïö©ÏïΩÍ¥Ä',
        'privacy': 'Í∞úÏù∏Ï†ïÎ≥¥ Ï≤òÎ¶¨Î∞©Ïπ®',
        'contact': 'Î¨∏Ïùò',
        'footer_note': 'Î∞îÎ°ú Ïã§Ìñâ Í∞ÄÎä•Ìïú Í∞ÄÏù¥ÎìúÎ•º Î™©ÌëúÎ°ú Ìï©ÎãàÎã§.',
        'aff_title': 'Ï∂îÏ≤ú',
        'copy': 'Î≥µÏÇ¨',
        'copied': 'Î≥µÏÇ¨Îê®',
        'short_value': '3Ï¥àÎ©¥ ÎÅù',
        'tool_input': 'ÏûÖÎ†•',
        'tool_input_hint': '(ÏÉÅÌô©ÏùÑ Î∂ôÏó¨ÎÑ£Í∏∞)',
        'tool_placeholder': 'Ïòà: ÎÇ†Ïßú/Ï†úÏïΩ/ÏãúÎèÑÌïú Í≤É/ÏõêÌïòÎäî Í≤É‚Ä¶',
        'tool_generate': 'ÏÉùÏÑ±',
        'tool_clear': 'ÏßÄÏö∞Í∏∞',
        'tool_tip': 'ÌåÅ: Ï†ïÌôïÌïú Ïò§Î•ò Î©îÏãúÏßÄÏôÄ ÏµúÍ∑º Î≥ÄÍ≤ΩÏ†êÏùÑ Ìè¨Ìï®ÌïòÏÑ∏Ïöî.'},
 'zh': {'home': 'Home',
        'about': 'About Us',
        'all_tools': 'All Tools',
        'language': 'ËØ≠Ë®Ä',
        'share': 'ÂàÜ‰∫´',
        'problems': 'Êú¨Â∑•ÂÖ∑ÂèØÂ∏ÆÂä©Ëß£ÂÜ≥ÁöÑÈóÆÈ¢ò',
        'tool': 'Â∑•ÂÖ∑',
        'quick_answer': 'ÁªìËÆ∫ÔºàÊúÄÂø´‰øÆÂ§çÊñπÂêëÔºâ',
        'causes': 'Â∏∏ËßÅÂéüÂõ†ÂàÜÁ±ª',
        'steps': 'Ê≠•È™§Ê∏ÖÂçï',
        'pitfalls': 'Â∏∏ËßÅÂùë‰∏éËßÑÈÅøÊñπÊ≥ï',
        'next': '‰ªçÊó†Ê≥ïËß£ÂÜ≥Êó∂',
        'faq': 'FAQ',
        'references': 'ÂèÇËÄÉÈìæÊé•',
        'supplement': 'Ë°•ÂÖÖËµÑÊñô',
        'related': 'Áõ∏ÂÖ≥Â∑•ÂÖ∑',
        'popular': 'ÁÉ≠Èó®Â∑•ÂÖ∑',
        'disclaimer': 'ÂÖçË¥£Â£∞Êòé',
        'terms': 'Êù°Ê¨æ',
        'privacy': 'ÈöêÁßÅÊîøÁ≠ñ',
        'contact': 'ËÅîÁ≥ª',
        'footer_note': 'Êèê‰æõÂèØËêΩÂú∞„ÄÅÂø´ÈÄü„ÄÅÂ∞äÈáçÁî®Êà∑ÁöÑÊéíÈöúÊåáÂçó„ÄÇ',
        'aff_title': 'Êé®Ëçê',
        'copy': 'Â§çÂà∂',
        'copied': 'Â∑≤Â§çÂà∂',
        'short_value': '3ÁßíÊêûÂÆö',
        'tool_input': 'ËæìÂÖ•',
        'tool_input_hint': 'ÔºàÁ≤òË¥¥‰Ω†ÁöÑÊÉÖÂÜµÔºâ',
        'tool_placeholder': '‰æãÂ¶ÇÔºöÊó•Êúü„ÄÅÈôêÂà∂„ÄÅÂ∑≤Â∞ùËØïÂÜÖÂÆπ„ÄÅÁõÆÊ†á‚Ä¶',
        'tool_generate': 'ÁîüÊàê',
        'tool_clear': 'Ê∏ÖÁ©∫',
        'tool_tip': 'ÊèêÁ§∫ÔºöËØ∑ÂåÖÂê´ÂÆåÊï¥Êä•Èîô‰ø°ÊÅØ‰ª•ÂèäÊúÄËøëÂèòÊõ¥ÁÇπ„ÄÇ'}}


def build_i18n_script(default_lang: str) -> str:
    """
    Returns a <script> block that:
      - Applies i18n to all [data-i18n] nodes
      - Applies i18n to placeholders via [data-i18n-placeholder]
      - Persists language/theme to localStorage
      - Supports light/dark toggle (class-based Tailwind dark mode)
    """
    i18n_json = json.dumps(I18N, ensure_ascii=False)
    langs_json = json.dumps(sorted(list(I18N.keys())), ensure_ascii=False)

    return f"""<script>
const I18N = {i18n_json};
const LANGS = {langs_json};

function t(lang, key) {{
  return (I18N[lang] && I18N[lang][key]) || (I18N[\"{default_lang}\"] && I18N[\"{default_lang}\"][key]) || key;
}}

function setLang(lang) {{
  if (!LANGS.includes(lang)) lang = \"{default_lang}\";
  document.documentElement.setAttribute(\"lang\", lang);
  localStorage.setItem(\"lang\", lang);

  document.querySelectorAll(\"[data-i18n]\").forEach(el => {{
    const key = el.getAttribute(\"data-i18n\");
    el.textContent = t(lang, key);
  }});

  document.querySelectorAll(\"[data-i18n-placeholder]\").forEach(el => {{
    const key = el.getAttribute(\"data-i18n-placeholder\");
    el.setAttribute(\"placeholder\", t(lang, key));
  }});

  document.querySelectorAll(\"[data-i18n-value]\").forEach(el => {{
    const key = el.getAttribute(\"data-i18n-value\");
    el.value = t(lang, key);
  }});
}}

function initLang() {{
  const saved = localStorage.getItem(\"lang\");
  const lang = saved || \"{default_lang}\";
  setLang(lang);
  const sel = document.getElementById(\"langSel\");
  if (sel) {{
    sel.value = lang;
    sel.addEventListener(\"change\", (e) => setLang(e.target.value));
  }}
}}

function setTheme(mode) {{
  if (mode === \"dark\") {{
    document.documentElement.classList.add(\"dark\");
  }} else {{
    document.documentElement.classList.remove(\"dark\");
  }}
  localStorage.setItem(\"theme\", mode);
}}

function initTheme() {{
  const saved = localStorage.getItem(\"theme\");
  const prefersDark = window.matchMedia && window.matchMedia(\"(prefers-color-scheme: dark)\").matches;
  const mode = saved || (prefersDark ? \"dark\" : \"light\");
  setTheme(mode);

  const btn = document.getElementById(\"themeBtn\");
  if (btn) {{
    btn.addEventListener(\"click\", () => {{
      const isDark = document.documentElement.classList.contains(\"dark\");
      setTheme(isDark ? \"light\" : \"dark\");
    }});
  }}
}}

document.addEventListener(\"DOMContentLoaded\", () => {{
  initTheme();
  initLang();
}});
</script>"""


# =============================================================================
# Content generation (quick answer, causes, steps, faq, article)
# =============================================================================
def build_quick_answer(category: str, keywords: List[str]) -> str:
    kw = ", ".join(keywords[:10])
    base = [
        "ÊúÄÁü≠„ÅßÈÄ≤„ÇÅ„ÇãÊñπÈáù„ÅØ„ÄåÂÜçÁèæÊù°‰ª∂„ÅÆÂõ∫ÂÆö ‚Üí ÂéüÂõ†„ÅÆÂàá„ÇäÂàÜ„Åë ‚Üí ÊúÄÂ∞èÂ§âÊõ¥ ‚Üí Ê§úË®º ‚Üí Ë®òÈå≤„Äç„Åß„Åô„ÄÇ",
        f"‰ªäÂõû„ÅÆ„Ç´„ÉÜ„Ç¥„É™„ÅØ„Äå{category}„Äç„Å™„ÅÆ„Åß„ÄÅ„Åæ„Åö„ÅØ‚Äú„Å©„Åì„ÅßÊ≠¢„Åæ„Å£„Å¶„ÅÑ„Çã„Åã‚Äù„ÇíÂ∞è„Åï„ÅèÂàÜËß£„Åó„Å¶Á¢∫Ë™ç„Åó„Åæ„Åô„ÄÇ",
        f"Ë¶≥Ê∏¨„Ç≠„Éº„ÉØ„Éº„Éâ: {kw}",
        "‰∏ã„ÅÆ„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà„ÅØ„ÄÅ‰∏ä„Åã„ÇâÈ†Ü„Å´ÊΩ∞„Åõ„Å∞‚Äú‰∫ãÊïÖÁéá‚Äù„Åå‰∏ã„Åå„ÇãÈ†ÜÁï™„Åß‰∏¶„Åπ„Å¶„ÅÑ„Åæ„Åô„ÄÇ",
    ]
    return "\n".join(base)


def build_causes(category: str) -> List[str]:
    common = {
        "Web/Hosting": [
            "DNS„ÅÆÂèçÊò†ÂæÖ„Å°ÔºàTTLÔºâ„ÇÑ„É¨„Ç≥„Éº„ÉâÁ®ÆÂà•„ÅÆË™§„ÇäÔºàA/CNAME/AAAA„ÅÆÊ∑∑Âú®Ôºâ",
            "HTTPS/Ë®ºÊòéÊõ∏„ÅÆËá™ÂãïÁô∫Ë°åÂæÖ„Å°„ÄÅ„É™„ÉÄ„Ç§„É¨„ÇØ„Éà„ÅÆ„É´„Éº„Éó",
            "„Éõ„Çπ„ÉÜ„Ç£„É≥„Ç∞ÂÅ¥„ÅÆË®≠ÂÆöÔºà„Ç´„Çπ„Çø„É†„Éâ„É°„Ç§„É≥„ÄÅ„Éë„Çπ„ÄÅ„Éô„Éº„ÇπURLÔºâ‰∏ç‰∏ÄËá¥",
            "„Ç≠„É£„ÉÉ„Ç∑„É•ÔºàCDN/„Éñ„É©„Ç¶„Ç∂/Service WorkerÔºâ„Å´„Çà„ÇãÂè§„ÅÑË°®Á§∫",
        ],
        "PDF/Docs": [
            "„Éï„Ç°„Ç§„É´„Çµ„Ç§„Ç∫/„Éö„Éº„Ç∏Êï∞‰∏äÈôê„Å´„Çà„ÇãÂ§±Êïó",
            "„Éï„Ç©„É≥„ÉàÂüã„ÇÅËæº„Åø„ÉªÊöóÂè∑Âåñ„Éª„Çπ„Ç≠„É£„É≥PDF„Åß„ÅÆ‰∫íÊèõÊÄßÂïèÈ°å",
            "Â§âÊèõÂÖàÂΩ¢Âºè„ÅÆÈÅ∏Êäû„Éü„ÇπÔºàÁîªÂÉèÂåñ„ÅåÂøÖË¶Å„Å™„ÅÆ„Å´„ÉÜ„Ç≠„Çπ„ÉàÂ§âÊèõ„ÇíÈÅ∏„Å∂Á≠âÔºâ",
            "„Éñ„É©„Ç¶„Ç∂„ÅÆ„É°„É¢„É™‰∏çË∂≥„ÉªÊã°ÂºµÊ©üËÉΩ„ÅÆÂπ≤Ê∏â",
        ],
        "Media": [
            "„Ç≥„Éº„Éá„ÉÉ„ÇØ‰∏ç‰∏ÄËá¥ÔºàH.264/H.265/AV1Ôºâ„ÇÑÈü≥Â£∞ÂΩ¢ÂºèÔºàAAC/OpusÔºâ",
            "„Éì„ÉÉ„Éà„É¨„Éº„Éà/Ëß£ÂÉèÂ∫¶‰∏äÈôê„Å´„Çà„Çã„Ç®„É©„Éº",
            "Á´ØÊú´ÊÄßËÉΩ„Éª„É°„É¢„É™‰∏çË∂≥„Å´„Çà„ÇãÂá¶ÁêÜËêΩ„Å°",
            "„Éï„Ç°„Ç§„É´Á†¥Êêç„Éª„Ç≥„É≥„ÉÜ„Éä‰∏çÊï¥ÂêàÔºàMP4/MKVÔºâ",
        ],
        "Data/Spreadsheets": [
            "Èñ¢Êï∞„ÅÆÂèÇÁÖßÁØÑÂõ≤„Ç∫„É¨„ÉªÁµ∂ÂØæÂèÇÁÖß/Áõ∏ÂØæÂèÇÁÖß„ÅÆ„Éü„Çπ",
            "Âå∫Âàá„ÇäÊñáÂ≠ó„ÉªÊñáÂ≠ó„Ç≥„Éº„Éâ„ÉªÊó•‰ªòÂΩ¢Âºè„ÅÆÂ∑ÆÔºàCSVÂèñ„ÇäËæº„ÅøÔºâ",
            "„Éï„Ç£„É´„Çø/„Éî„Éú„ÉÉ„Éà„ÅÆÊõ¥Êñ∞Âøò„Çå",
            "ÂÖ±ÊúâË®≠ÂÆö/Ê®©Èôê„ÅßÁ∑®ÈõÜ„ÅåÂèçÊò†„Åï„Çå„Å™„ÅÑ",
        ],
        "Security/Privacy": [
            "Ê®©ÈôêÔºàOAuth/„Éà„Éº„ÇØ„É≥ÔºâÊúüÈôêÂàá„Çå„Éª„Çπ„Ç≥„Éº„Éó‰∏çË∂≥",
            "Cookie/ËøΩË∑°„Éñ„É≠„ÉÉ„ÇØ„Åß„É≠„Ç∞„Ç§„É≥„ÅåÂ£ä„Çå„Çã",
            "2FA„ÇÑÁ´ØÊú´Ë™çË®º„ÅÆ‰∏ç‰∏ÄËá¥",
            "ÂÅΩ„Çµ„Ç§„Éà/„Éï„Ç£„ÉÉ„Ç∑„É≥„Ç∞„Éª„Çª„Ç≠„É•„É™„ÉÜ„Ç£„ÇΩ„Éï„Éà„ÅÆË™§Ê§úÁü•",
        ],
        "AI/Automation": [
            "API„Ç≠„Éº/Ê®©Èôê‰∏çË∂≥„ÄÅ„É¨„Éº„ÉàÂà∂Èôê„ÄÅ„É¢„Éá„É´Âêç„ÅÆ‰∏ç‰∏ÄËá¥",
            "ÂÖ•Âäõ„ÅåÊõñÊòß„ÅßÂá∫Âäõ„ÅåÂÆâÂÆö„Åó„Å™„ÅÑÔºà‰ªïÊßò„ÅåÊè∫„Çå„Å¶„ÅÑ„ÇãÔºâ",
            "„Éï„Ç°„Ç§„É´/„Éë„Çπ„ÅÆ‰∏äÊõ∏„Åç‰∫ãÊïÖ„ÄÅË°ùÁ™ÅÊôÇ„ÅÆÂá¶ÁêÜÊºè„Çå",
            "„É≠„Ç∞‰∏çË∂≥„ÅßÂéüÂõ†ÁâπÂÆö„ÅåÈÅÖ„Çå„Çã",
        ],
        "Travel/Planning": [
            "ÁõÆÁöÑ„ÉªÊó•Êï∞„ÉªÁßªÂãïÂà∂Á¥Ñ„ÅåÊ±∫„Åæ„Å£„Å¶„Åä„Çâ„Åö„ÄÅÊóÖÁ®ã„ÅåÁô∫Êï£„Åô„Çã",
            "ÁßªÂãïÊôÇÈñì„ÅÆË¶ãÁ©ç„ÇÇ„Çä„ÅåÁîò„Åè„ÄÅË©∞„ÇÅËæº„Åø„Åô„Åé„Å´„Å™„Çã",
            "ÊåÅ„Å°Áâ©„Åå‚ÄúÁèæÂú∞Ë™øÈÅî„Åß„Åç„ÇãÁâ©/„Åß„Åç„Å™„ÅÑÁâ©‚Äù„ÅßÂàÜ„Åë„Çâ„Çå„Å¶„ÅÑ„Å™„ÅÑ",
            "‰∫àÁÆóÈÖçÂàÜÔºàÂÆø/‰∫§ÈÄö/È£ü/‰∫àÂÇôË≤ªÔºâ„ÅåÊõñÊòß„Åß‰∏çÂÆâ„ÅåÊÆã„Çã",
        ],
        "Food/Cooking": [
            "ÁåÆÁ´ã„ÅåÂÖà„Å´Ê±∫„Åæ„Çâ„Åö„ÄÅË≤∑„ÅÑÁâ©„ÅåËø∑Â≠ê„Å´„Å™„Çã",
            "‰Ωú„ÇäÁΩÆ„Åç„ÅÆ‚Äú‰øùÂ≠òÊó•Êï∞/Ê∏©„ÇÅÁõ¥„Åó‚Äù„ÇíËÄÉ„Åà„Åö„Å´Âõû„Çâ„Å™„ÅÑ",
            "Ê†ÑÈ§ä„Éê„É©„É≥„ÇπÔºà„Åü„Çì„Å±„ÅèË≥™/ÈáéËèú/ÁÇ≠Ê∞¥ÂåñÁâ©Ôºâ„ÅÆÂÅè„Çä",
            "ÊôÇÈñì„ÅÆË¶ãÁ©ç„ÇÇ„Çä‰∏çË∂≥„ÅßÁµêÂ±ÄÂ§ñÈ£ü„Å´„Å™„Çã",
        ],
        "Health/Fitness": [
            "Áù°Áú†/È£ü‰∫ã/ÈÅãÂãï„ÅÆ„Å©„Çå„Åå„Éú„Éà„É´„Éç„ÉÉ„ÇØ„ÅãÂàÜ„Åã„Å£„Å¶„ÅÑ„Å™„ÅÑ",
            "ÁøíÊÖ£Âåñ„ÅÆÂçò‰Ωç„ÅåÂ§ß„Åç„Åô„Åé„Å¶Á∂ôÁ∂ö„Åß„Åç„Å™„ÅÑ",
            "Âº∑Â∫¶„ÅåÈ´ò„Åô„Åé„Å¶Áñ≤Âä¥‚Üí‰∏≠Êñ≠„ÅÆ„É´„Éº„Éó",
            "Ë®òÈå≤„Åå„Å™„Åè„ÄÅÊîπÂñÑÁÇπ„ÅåË¶ã„Åà„Å™„ÅÑ",
        ],
        "Study/Learning": [
            "Âæ©Áøí„Çø„Ç§„Éü„É≥„Ç∞„ÅåÂõ∫ÂÆö„Åï„Çå„Åö„ÄÅÂøòÂç¥„ÅßÂäπÁéá„ÅåËêΩ„Å°„Çã",
            "ÊïôÊùê„ÅåÂ§ö„Åô„Åé„Å¶ÂÑ™ÂÖàÈ†Ü‰Ωç„ÅåÊ±∫„Åæ„Çâ„Å™„ÅÑ",
            "ÁõÆÊ®ô„ÅåÊäΩË±°ÁöÑ„Åß„ÄÅ‰ªäÊó•„ÇÑ„Çã„Åì„Å®„Å´ËêΩ„Å°„Å™„ÅÑ",
            "ÈõÜ‰∏≠Áí∞Â¢É„ÅåÊï¥„Å£„Å¶„ÅÑ„Å™„ÅÑÔºàÈÄöÁü•/Â†¥ÊâÄ/ÊôÇÈñìÂ∏ØÔºâ",
        ],
        "Money/Personal Finance": [
            "Âõ∫ÂÆöË≤ª„ÉªÂ§âÂãïË≤ª„ÉªÁâπÂà•Ë≤ª„ÅÆÂå∫Âà•„Åå„Å™„Åè„ÄÅÂéüÂõ†„ÅåË¶ã„Åà„Å™„ÅÑ",
            "ÊâãÊï∞Êñô/ËøîÈáëÊù°‰ª∂„ÅÆÁ¢∫Ë™ç‰∏çË∂≥",
            "ÊîØÊâï„ÅÑÊó•„ÉªÂºï„ÅçËêΩ„Å®„ÅóÊó•„Åå„Ç∫„É¨„Å¶Ë≥áÈáëÁπ∞„Çä„ÅåËã¶„Åó„ÅÑ",
            "ÊØîËºÉËª∏ÔºàÁ∑èÈ°ç/Âà©‰æøÊÄß/„É™„Çπ„ÇØÔºâ„ÅåÊõñÊòß",
        ],
        "Career/Work": [
            "ËÅ∑ÂãôË¶ÅÁ¥Ñ„ÅåÈï∑„Åô„Åé„Å¶Ë¶ÅÁÇπ„ÅåÂüã„ÇÇ„Çå„Çã",
            "ÂÆüÁ∏æ„Åå‚ÄúÊï∞Â≠ó‚Äù„ÅßÊõ∏„Åë„Å¶„Åä„Çâ„ÅöÂº∑„Åø„Åå‰ºù„Çè„Çâ„Å™„ÅÑ",
            "Èù¢Êé•ÊÉ≥ÂÆöÂïèÁ≠î„ÅåÁî®ÊÑè„Åï„Çå„Å¶„Åä„Çâ„ÅöË©∞„Åæ„Çã",
            "ÂøúÂãüÂÖà„Åî„Å®„ÅÆ„Ç´„Çπ„Çø„É†„Åå‰∏çË∂≥",
        ],
        "Relationships/Communication": [
            "‰ºù„Åà„Åü„ÅÑ„Åì„Å®„ÅåÂ§ö„Åè„ÄÅÊñá„ÅåÈï∑„Åè„Å™„Å£„Å¶Ë™§Ëß£„Åï„Çå„Çã",
            "Áõ∏Êâã„ÅÆÊ∏©Â∫¶ÊÑü„Å´Âêà„Çè„Åõ„ÅüË®Ä„ÅÑÂõû„Åó„Åå‰∏çË∂≥",
            "Êñ≠„ÇäÊñπ/„ÅäÈ°ò„ÅÑ„ÅÆÂûã„Åå„Å™„ÅèÊ∞ó„Åæ„Åö„Åè„Å™„Çã",
            "Ëøî‰ø°„Çø„Ç§„Éü„É≥„Ç∞„Åå‰∏çÂÆâ„ÅßÁ©∫Âõû„Çä„Åô„Çã",
        ],
        "Home/Life Admin": [
            "„ÇÑ„Çã„Åì„Å®„ÅÆÊ£öÂç∏„Åó„Åå„Å™„Åè„ÄÅÊäú„ÅëÊºè„Çå„ÅåÂá∫„Çã",
            "ÊúüÈôê„ÉªÊèêÂá∫ÂÖà„ÉªÂøÖË¶ÅÊõ∏È°û„ÅåÊï£„Çâ„Å∞„Å£„Å¶„ÅÑ„Çã",
            "Áâá‰ªò„Åë„ÅÆÁØÑÂõ≤„ÅåÂ∫É„Åô„Åé„Å¶ÈÄ≤„Åæ„Å™„ÅÑ",
            "„É´„Éº„ÉÜ„Ç£„É≥Âåñ„Åß„Åç„ÅöÊØéÂõû„Çº„É≠„Åã„ÇâËÄÉ„Åà„Çã",
        ],
        "Shopping/Products": [
            "ÊØîËºÉËª∏Ôºà‰æ°Ê†º/‰øùË®º/„Çµ„Ç§„Ç∫/ËÄê‰πÖ/Áî®ÈÄîÔºâ„ÅåÂÆöÁæ©„Åß„Åç„Å¶„ÅÑ„Å™„ÅÑ",
            "„É¨„Éì„É•„Éº„ÅÆË™≠„ÅøÊñπ„ÅåÂÅè„Çä„ÄÅÁµêË´ñ„ÅåÂá∫„Å™„ÅÑ",
            "ÂøÖË¶ÅÂçÅÂàÜ„ÅÆ„Çπ„Éö„ÉÉ„ÇØ„ÅåÂàÜ„Åã„Çâ„Å™„ÅÑ",
            "Ë≤∑„ÅÜ„Çø„Ç§„Éü„É≥„Ç∞Ôºà„Çª„Éº„É´/ËøîÂìÅÂèØÂê¶Ôºâ„Åå‰∏çÊòé",
        ],
        "Events/Leisure": [
            "ÂÄôË£ú„ÅåÂ§ö„Åè„ÄÅÂÑ™ÂÖàÈ†Ü‰Ωç„ÅåÊ±∫„Åæ„Çâ„Å™„ÅÑ",
            "Â§©Ê∞ó„ÉªÊ∑∑Èõë„ÉªÁßªÂãïÊôÇÈñì„ÅÆË¶ãÁ©ç„ÇÇ„Çä‰∏çË∂≥",
            "ÂΩìÊó•„ÅÆÊåÅ„Å°Áâ©/‰∫àÁ¥Ñ/ÊîØÊâï„ÅÑ„Åå‰∏çÂÆâ",
            "ÂêåË°åËÄÖ„ÅÆÂ∏åÊúõ„ÅåÊï¥ÁêÜ„Åß„Åç„Å¶„ÅÑ„Å™„ÅÑ",
        ],
    }
    return common.get(category, [
        "ÂÖ•Âäõ„ÉªÂâçÊèêÊù°‰ª∂„ÅÆ„Ç∫„É¨ÔºàÊÉ≥ÂÆö„Å®ÂÆüÈöõ„ÅåÈÅï„ÅÜÔºâ",
        "Ê®©Èôê/Ë®≠ÂÆö/„Éê„Éº„Ç∏„Éß„É≥„ÅÆ‰∏ç‰∏ÄËá¥",
        "„Ç≠„É£„ÉÉ„Ç∑„É•„ÇÑÂèçÊò†ÂæÖ„Å°",
        "ÂéüÂõ†„ÅåÂâçÊÆµ„Å´„ÅÇ„Çã„ÅÆ„Å´„ÄÅË¶ã„Åà„Å¶„ÅÑ„ÇãÁîªÈù¢„ÅßÊ±∫„ÇÅÊâì„Å°„Åó„Å¶„ÅÑ„Çã",
    ])


def build_steps(category: str) -> List[str]:
    """
    Step-by-step checklist generator.
    NOTE: „Åì„ÅÆÈñ¢Êï∞„ÅØ SyntaxError „ÅÆÂéüÂõ†„Å´„Å™„Çä„ÇÑ„Åô„ÅÑ„ÅÆ„Åß„ÄÅ
          Êã¨Âºß„ÇÑ„ÇØ„Ç©„Éº„Éà„ÅÆÈñâ„ÅòÂøò„Çå„ÅåËµ∑„Åç„Å™„ÅÑ ‚ÄúÂÆâÂÖ®„Å™Âõ∫ÂÆöÂΩ¢‚Äù „Å´„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
    """
    steps: List[str] = [
        "ÂÜçÁèæÊù°‰ª∂„ÇíÂõ∫ÂÆö„Åô„ÇãÔºàÂêå„ÅòÂÖ•Âäõ„ÉªÂêå„ÅòÊâãÈ†Ü„ÉªÂêå„ÅòÁ´ØÊú´/„Éñ„É©„Ç¶„Ç∂„ÅßÂÜçÁèæÔºâ",
        "Ë°®Á§∫/„É≠„Ç∞„Çí„Åù„ÅÆ„Åæ„Åæ‰øùÂ≠òÔºà„Ç≥„Éî„Éö/„Çπ„ÇØ„Ç∑„Éß„ÄÅÊôÇÂàª„ÇÇÊÆã„ÅôÔºâ",
        "ÂΩ±ÈüøÁØÑÂõ≤„ÅåÂ∞è„Åï„ÅÑÈ†Ü„Å´Á¢∫Ë™çÔºàÁ¢∫Ë™ç‚ÜíË™≠„ÅøÂèñ„Çä‚ÜíÊúÄÂ∞èÂ§âÊõ¥‚ÜíÊ§úË®ºÔºâ",
        "Áõ¥„Å£„Åü„ÇâÂ∑ÆÂàÜ„ÇíË®òÈå≤„Åó„ÄÅÂÜçÁô∫Èò≤Ê≠¢„ÉÅ„Çß„ÉÉ„ÇØ„Çí‰Ωú„ÇãÔºàÊ¨°Âõû3ÂàÜÂæ©Êóß„ÅåÁõÆÊ®ôÔºâ",
    ]

    if category in ["Web/Hosting", "AI/Automation"]:
        steps += [
            "‰∏äÊõ∏„ÅçÁ¶ÅÊ≠¢„ÇíÂº∑Âà∂„Åô„ÇãÔºàË°ùÁ™Å„ÅØ -2/-3„ÄÅÂáçÁµê„Éë„Çπ„ÅØËß¶„Çâ„Å™„ÅÑÔºâ",
            "„É≠„Ç∞Á≤íÂ∫¶„Çí‰∏ä„Åí„ÇãÔºàHTTP„Çπ„ÉÜ„Éº„Çø„Çπ/‰æãÂ§ñ/„É¨„Çπ„Éù„É≥„ÇπÂÖàÈ†≠Ôºâ",
        ]

    if category == "Travel/Planning":
        steps += [
            "Êó•Êï∞„ÉªÂá∫Áô∫/Â∏∞ÂÆÖÊôÇÂàª„ÉªÁµ∂ÂØæ„Å´„ÇÑ„Çä„Åü„ÅÑ„Åì„Å®Ôºà3„Å§Ôºâ„ÇíÂÖà„Å´Âõ∫ÂÆö",
            "ÁßªÂãïÊôÇÈñì„ÇíÂÖà„Å´ÁΩÆ„ÅÑ„Å¶„ÄÅÊÆã„Çä„Å´Ë¶≥ÂÖâ„ÇíÂÖ•„Çå„ÇãÔºàË©∞„ÇÅËæº„ÅøÈò≤Ê≠¢Ôºâ",
            "ÊåÅ„Å°Áâ©„Çí„ÄåÂøÖÈ†à/ÁèæÂú∞Ë™øÈÅî/‰∫àÂÇô„Äç„Å´ÂàÜ„Åë„Å¶„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„ÉàÂåñ",
            "‰∫àÁÆó„Çí„ÄåÂÆø/‰∫§ÈÄö/È£ü/Ë¶≥ÂÖâ/‰∫àÂÇôË≤ª„Äç„Å´Ââ≤„Å£„Å¶‰∏äÈôê„ÇíÊ±∫„ÇÅ„Çã",
        ]

    if category == "Food/Cooking":
        steps += [
            "‰∏ªËèú„ÇíÂÖà„Å´Ê±∫„ÇÅ„ÇãÔºà3„Äú5ÂÄãÔºâ‚ÜíÂâØËèú‚Üí‰∏ªÈ£ü„ÅÆÈ†Ü„ÅßÊ±∫„ÇÅ„Çã",
            "Ë≤∑„ÅÑÁâ©„É™„Çπ„Éà„Çí„Ç´„ÉÜ„Ç¥„É™Âà•ÔºàËÇâ/ÈáéËèú/Ë™øÂë≥Êñô‚Ä¶Ôºâ„Å´Âá∫„Åô",
            "‰Ωú„ÇäÁΩÆ„Åç„ÅØ‰øùÂ≠òÊó•Êï∞„Éô„Éº„Çπ„ÅßÂõû„ÅôÔºàÂÖà„Å´Ê∂àË≤ªÈ†Ü„ÇíÊ±∫„ÇÅ„ÇãÔºâ",
            "Ë™øÁêÜ„ÅØÂêåÊôÇÈÄ≤Ë°å„Åó„ÇÑ„Åô„ÅÑÈ†Ü„Å´‰∏¶„Åπ„ÇãÔºàÁÑº„Åè/Ëåπ„Åß„Çã/Âàá„ÇãÔºâ",
        ]

    if category == "Health/Fitness":
        steps += [
            "„Åæ„ÅöÁù°Áú†„ÇíÂõ∫ÂÆöÔºàÂ∞±ÂØù/Ëµ∑Â∫ä„ÅÆÊôÇÂàª„ÇíÂÖà„Å´Ê±∫„ÇÅ„ÇãÔºâ",
            "ÈÅãÂãï„ÅØÊúÄÂ∞èÂçò‰Ωç„Åã„ÇâÔºà‰æãÔºöËÖïÁ´ã„Å¶5Âõû/Êï£Ê≠©10ÂàÜÔºâ",
            "ÈÄ±„ÅÆÂõûÊï∞‚ÜíÂº∑Â∫¶„ÅÆÈ†Ü„Åß‰∏ä„Åí„ÇãÔºà„ÅÑ„Åç„Å™„ÇäÂº∑Â∫¶„ÅØ‰∏ä„Åí„Å™„ÅÑÔºâ",
            "Ë®òÈå≤„ÅØ1È†ÖÁõÆ„Å†„ÅëÔºà‰ΩìÈáç/Ê≠©Êï∞/Áù°Áú†„Å™„Å©Ôºâ„Åã„ÇâÈñãÂßã",
        ]

    if category == "Study/Learning":
        steps += [
            "ÁõÆÊ®ô„Çí„Äå‰ªäÈÄ±„ÅÆÈáè„Äç‚Üí„Äå‰ªäÊó•„ÅÆÈáè„Äç„Å´Ââ≤„ÇãÔºàÊúÄÂ∞èÂçò‰Ωç„Çí‰Ωú„ÇãÔºâ",
            "Âæ©Áøí„ÅØÁøåÊó•/3Êó•Âæå/7Êó•Âæå„ÅÆÂõ∫ÂÆöÊû†„ÅßÂõû„Åô",
            "ÊïôÊùê„ÅØÂêåÊôÇ„Å´2„Å§„Åæ„ÅßÔºàÂ¢ó„ÇÑ„Åô„Åª„Å©Ëø∑„ÅÜÔºâ",
            "ÈõÜ‰∏≠„ÅØÁí∞Â¢É„Åß‰Ωú„ÇãÔºàÈÄöÁü•OFF/Â†¥ÊâÄÂõ∫ÂÆö/ÈñãÂßã„ÅÆÂÑÄÂºèÔºâ",
        ]

    if category == "Money/Personal Finance":
        steps += [
            "Âõ∫ÂÆöË≤ª/Â§âÂãïË≤ª/ÁâπÂà•Ë≤ª„Å´ÂàÜ„Åë„Å¶„ÄÅ„Åæ„ÅöÂõ∫ÂÆöË≤ª„Åã„ÇâÊúÄÈÅ©Âåñ",
            "ÊâãÊï∞Êñô/ËøîÈáëÊù°‰ª∂/Ëß£Á¥ÑÊù°‰ª∂„Çí‚ÄúÂÖà„Å´‚ÄùÁ¢∫Ë™ç„Åó„Å¶‰∫ãÊïÖ„ÇíÈò≤„Åê",
            "ÊîØÊâï„ÅÑÊó•„ÉªÂºï„ÅçËêΩ„Å®„ÅóÊó•„Çí„Ç´„É¨„É≥„ÉÄ„Éº„Å´Âõ∫ÂÆöÔºà„Ç∫„É¨„ÅßË©∞„Åæ„Å™„ÅÑÔºâ",
            "ÊØîËºÉËª∏ÔºàÁ∑èÈ°ç/Âà©‰æøÊÄß/„É™„Çπ„ÇØÔºâ„Çí1Êûö„Å´„Åæ„Å®„ÇÅ„Å¶Ê±∫„ÇÅÂàá„Çã",
        ]

    if category == "Career/Work":
        steps += [
            "ÂÆüÁ∏æ„ÅØÊï∞Â≠ó„ÅßÊõ∏„ÅèÔºà‰æãÔºöÊîπÂñÑÁéá/‰ª∂Êï∞/ÊúüÈñì/ÂΩπÂâ≤Ôºâ",
            "ËÅ∑ÂãôË¶ÅÁ¥Ñ„ÅØ3Ë°å„ÅßÁµêË´ñ‚ÜíÊ†πÊã†‚ÜíÂÜçÁèæÊÄß„ÅÆÈ†Ü",
            "ÂøúÂãüÂÖà„Åî„Å®„Å´Ë¶ÅÁÇπ„Å†„ÅëÂ∑Æ„ÅóÊõø„Åà„ÇãÔºàÂÖ®ÈÉ®„ÇíÊõ∏„ÅçÊèõ„Åà„Å™„ÅÑÔºâ",
            "Èù¢Êé•„ÅØÊÉ≥ÂÆöË≥™Âïè„ÇíÂÖà„Å´ÊΩ∞„ÅôÔºàËá™Â∑±Á¥π‰ªã/ÂøóÊúõÂãïÊ©ü/Âº∑„Åø/Âº±„ÅøÔºâ",
        ]

    if category == "Relationships/Communication":
        steps += [
            "Êñá„ÇíÁü≠„Åè„Åô„ÇãÔºà1Êñá1Ë¶ÅÁÇπ„ÄÅ‰ΩôË®à„Å™ÂâçÁΩÆ„Åç„ÇíÂâä„ÇãÔºâ",
            "„ÅäÈ°ò„ÅÑ/Êñ≠„Çä/„ÅäÁ§º„ÅÆÂûã„Çí‰Ωø„ÅÜÔºàÊØéÂõû„Çº„É≠„Åã„ÇâËÄÉ„Åà„Å™„ÅÑÔºâ",
            "Áõ∏Êâã„ÅÆÊ∏©Â∫¶ÊÑü„Å´Âêà„Çè„Åõ„Å¶ÊÉÖÂ†±Èáè„ÇíË™øÊï¥„Åô„Çã",
            "Ëøî‰ø°„Åå‰∏çÂÆâ„Å™„Çâ‚ÄúÈÅ∏ÊäûËÇ¢‚Äù„ÅßËøî„ÅôÔºàA„ÅãB„ÄÅ„Å©„Å£„Å°„Åå„ÅÑ„ÅÑÔºüÂΩ¢ÂºèÔºâ",
        ]

    if category == "Home/Life Admin":
        steps += [
            "„ÇÑ„Çã„Åì„Å®„ÇíÊ£öÂç∏„Åó‚ÜíÊúüÈôê‚ÜíÊèêÂá∫ÂÖà‚ÜíÂøÖË¶ÅÊõ∏È°û„ÅÆÈ†Ü„ÅßÊï¥ÁêÜ",
            "„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà„ÅØ‚ÄúÊèêÂá∫Âçò‰Ωç‚Äù„Åß‰Ωú„ÇãÔºàÊõ∏È°û1„Å§=1È†ÖÁõÆÔºâ",
            "Áâá‰ªò„Åë„ÅØÁØÑÂõ≤„ÇíÂ∞è„Åï„ÅèÂàá„ÇãÔºàÂºï„ÅçÂá∫„Åó1„Å§„Å™„Å©Ôºâ",
            "„É´„Éº„ÉÜ„Ç£„É≥„ÅØÂõ∫ÂÆöÊôÇÂàª„Å´ÁΩÆ„ÅèÔºàÊØéÈÄ±/ÊØéÊúà„ÅßÁπ∞„ÇäËøî„ÅóÔºâ",
        ]

    if category == "Shopping/Products":
        steps += [
            "ÊØîËºÉËª∏„ÇíÊ±∫„ÇÅ„ÇãÔºà‰æ°Ê†º/‰øùË®º/„Çµ„Ç§„Ç∫/ËÄê‰πÖ/Áî®ÈÄîÔºâ",
            "ÂøÖË¶ÅÂçÅÂàÜ„Çπ„Éö„ÉÉ„ÇØ„ÇíÂÖà„Å´Á¢∫ÂÆöÔºà‰∏ä‰Ωç‰∫íÊèõ„ÇíËøΩ„Çè„Å™„ÅÑÔºâ",
            "„É¨„Éì„É•„Éº„ÅØ‰ΩéË©ï‰æ°‚Üí‰∏≠Ë©ï‰æ°‚ÜíÈ´òË©ï‰æ°„ÅÆÈ†Ü„ÅßË™≠„ÇÄÔºàÂú∞Èõ∑ÂõûÈÅøÔºâ",
            "ËøîÂìÅÊù°‰ª∂„Å®Âà∞ÁùÄÊó•„ÇíÊúÄÂæå„Å´Á¢∫Ë™ç„Åó„Å¶Ë≥ºÂÖ•",
        ]

    if category == "Events/Leisure":
        steps += [
            "ÂÄôË£ú„Çí3„Å§„Åæ„Åß„Å´Áµû„ÇãÔºàÂ¢ó„ÇÑ„Åô„Åª„Å©Ê±∫„ÇÅ„Çâ„Çå„Å™„ÅÑÔºâ",
            "Â§©Ê∞ó„ÉªÊ∑∑Èõë„ÉªÁßªÂãïÊôÇÈñì„ÇíÂÖà„Å´ÁΩÆ„ÅèÔºàÂΩìÊó•Â¥©Â£ä„ÇíÈò≤„ÅêÔºâ",
            "‰∫àÁ¥Ñ/ÊîØÊâï„ÅÑ/ÊåÅ„Å°Áâ©„ÇíÂâçÊó•„Åæ„Åß„Å´Á¢∫ÂÆö",
            "ÂêåË°åËÄÖ„Åå„ÅÑ„Çã„Å™„ÇâÂ∏åÊúõ„Çí1Êûö„Å´„Åæ„Å®„ÇÅ„Å¶ÂêàÊÑè",
        ]

    # ‰ΩôÂàÜ„Å´Â¢ó„Åà„Åô„Åé„Å™„ÅÑ„Çà„ÅÜ„Å´‰∏äÈôê
    return steps[:28]



def build_pitfalls(category: str) -> List[str]:
    pitfalls = [
        "‰∏ÄÊ∞ó„Å´Ë§áÊï∞ÁÆáÊâÄ„ÇíÂ§â„Åà„Å¶„Åó„Åæ„ÅÑ„ÄÅ„Å©„Çå„ÅåÂéüÂõ†„ÅãÂàÜ„Åã„Çâ„Å™„Åè„Å™„Çã",
        "ÂèçÊò†ÂæÖ„Å°ÔºàDNS/„Ç≠„É£„ÉÉ„Ç∑„É•Ôºâ„ÇíÁÑ°Ë¶ñ„Åó„Å¶ÁÑ¶„Å£„Å¶„Åï„Çâ„Å´Â£ä„Åô",
        "„É≠„Ç∞/„É°„É¢„ÇíÂèñ„Çâ„Åö„Å´Ë©¶Ë°åÂõûÊï∞„Å†„ÅëÂ¢ó„ÇÑ„ÅôÔºàÂæå„ÅßÂæ©Êóß‰∏çËÉΩ„Å´„Å™„ÇãÔºâ",
        "‚Äú„ÅÑ„ÅæË¶ã„Åà„Å¶„ÅÑ„ÇãÁîªÈù¢‚Äù„ÅåÂéüÂõ†„Å†„Å®Ê±∫„ÇÅ„Å§„Åë„ÇãÔºàÂâçÊÆµ„ÅåÂéüÂõ†„ÅÆ„Åì„Å®„ÅåÂ§ö„ÅÑÔºâ",
    ]
    if category in ["Web/Hosting", "AI/Automation"]:
        pitfalls.append("Êó¢Â≠òURL„ÇÑÂáçÁµêÈ†òÂüüÔºà/hub/Ôºâ„Çí‰∏äÊõ∏„Åç„Åó„Å¶Ë≥áÁî£„ÇíÂ£ä„ÅôÔºàÁµ∂ÂØæÁ¶ÅÊ≠¢Ôºâ")
    if category in ["Travel/Planning", "Food/Cooking", "Shopping/Products"]:
        pitfalls.append("ÊØîËºÉËª∏„ÅåÊõñÊòß„Å™„Åæ„ÅæÊÉÖÂ†±ÂèéÈõÜ„ÅóÁ∂ö„Åë„Å¶Ê±∫Êñ≠„Åß„Åç„Å™„ÅÑ")
    if category in ["Health/Fitness", "Study/Learning"]:
        pitfalls.append("ÊúÄÂàù„Åã„ÇâÈáè„ÇíÁõõ„Çä„Åô„Åé„Å¶„ÄÅÁ∂ö„Åã„ÅöËá™Â∑±Â´åÊÇ™„Å´„Å™„Çã")
    return pitfalls


def build_next_actions(category: str) -> List[str]:
    nxt = [
        "Âà•ÁµåË∑Ø„ÅßÂêå„ÅòÁµêÊûú„ÅåÂá∫„Çã„ÅãÁ¢∫Ë™çÔºàÂà•Á´ØÊú´/Âà•ÂõûÁ∑ö/Âà•„Éñ„É©„Ç¶„Ç∂Ôºâ",
        "„É≠„Ç∞/„É°„É¢„ÅÆÁ≤íÂ∫¶„Çí‰∏ä„Åí„ÇãÔºàÂ§±ÊïóÊôÇ„ÅÆÊù°‰ª∂„Å®Â∑ÆÂàÜ„ÇíÊÆã„ÅôÔºâ",
        "‚ÄúÂÖÉ„Å´Êàª„Åõ„ÇãÂΩ¢‚Äù„ÅßÊÆµÈöéÁöÑ„Å´„É≠„Éº„É´„Éê„ÉÉ„ÇØÔºàÂ§âÊõ¥ÂâçÂæå„ÅÆÂ∑ÆÂàÜ„ÇíÊÆã„ÅôÔºâ",
        "Âêå„ÅòÂ§±Êïó„ÇíÁπ∞„ÇäËøî„Åï„Å™„ÅÑ„Çà„ÅÜ„ÄÅ„ÉÅ„Çß„ÉÉ„ÇØÈ†ÖÁõÆ„ÇíÂõ∫ÂÆöÂåñ„Åô„Çã",
    ]
    if category == "Security/Privacy":
        nxt.append("ÊÄ™„Åó„ÅÑ„É™„É≥„ÇØ/Ë™çË®ºÁîªÈù¢„ÅØË∏è„Åæ„Å™„ÅÑ„ÄÇÂÖ¨Âºè„Éâ„É°„Ç§„É≥„Å®Ë®ºÊòéÊõ∏„ÇíÂÜçÁ¢∫Ë™ç")
    if category in ["Travel/Planning", "Money/Personal Finance"]:
        nxt.append("ÊúÄÊÇ™„Ç±„Éº„ÇπÔºàÂª∂Ê≥ä/„Ç≠„É£„É≥„Çª„É´/ÊâãÊï∞ÊñôÔºâ„ÇíÂÖà„Å´ÊÉ≥ÂÆö„Åó„Å¶‰∫àÂÇôË≤ª„Éª‰ª£ÊõøÊ°à„ÇíÁî®ÊÑè")
    return nxt


def build_faq(category: str) -> List[Tuple[str, str]]:
    base = [
        ("What should I check first?", "Fix the conditions: steps, expected result, actual result, and what changed recently."),
        ("How do I know if it‚Äôs just cache / stale data?", "Try private mode or a different device. If it changes, cache is likely involved."),
        ("What‚Äôs the safest order to troubleshoot?", "Confirm ‚Üí read-only checks ‚Üí one small change ‚Üí verify ‚Üí write down the diff."),
        ("What should I do after it works?", "Save the diff + a quick checklist so the next recovery is under 3 minutes."),
        ("How should I share this problem with someone?", "Include steps to reproduce, expected vs actual, logs/screenshots, and environment."),
    ]
    if category == "Web/Hosting":
        base.append(("How long can DNS propagation take?", "It depends on TTL and resolvers. Confirm from a third-party DNS lookup too."))
    if category == "Travel/Planning":
        base.append(("How do I avoid overpacking?", "Split items into: must-have, can-buy-there, and optional backups. Then cut optional."))
    if category == "Shopping/Products":
        base.append(("How do I stop endless comparing?", "Limit to 3 options, pick 3 criteria, then decide using total cost + return policy."))
    # ensure >= MIN_FAQ
    return base[: max(MIN_FAQ, 5)]


def supplemental_resources_for_category(category: str) -> List[str]:
    base: Dict[str, List[str]] = {
        "Web/Hosting": [
            "https://pages.github.com/",
            "https://letsencrypt.org/docs/",
            "https://developer.mozilla.org/en-US/docs/Learn/Common_questions/Web_mechanics/What_is_a_domain_name",
        ],
        "Security/Privacy": [
            "https://owasp.org/www-project-top-ten/",
            "https://developer.mozilla.org/en-US/docs/Web/HTTP/Cookies",
            "https://en.wikipedia.org/wiki/Phishing",
        ],
        "PDF/Docs": [
            "https://www.adobe.com/acrobat/resources/what-is-pdf.html",
            "https://en.wikipedia.org/wiki/PDF",
            "https://developer.mozilla.org/en-US/docs/Web/API/File",
        ],
        "Media": [
            "https://ffmpeg.org/documentation.html",
            "https://en.wikipedia.org/wiki/Video_codec",
            "https://developer.mozilla.org/en-US/docs/Web/Media",
        ],
        "Data/Spreadsheets": [
            "https://support.google.com/docs/?hl=en#topic=1382883",
            "https://support.microsoft.com/excel",
            "https://en.wikipedia.org/wiki/Comma-separated_values",
        ],
        "AI/Automation": [
            "https://docs.github.com/en/actions",
            "https://en.wikipedia.org/wiki/Cron",
            "https://en.wikipedia.org/wiki/Automation",
        ],
        "Travel/Planning": [
            "https://en.wikipedia.org/wiki/Travel_itinerary",
            "https://en.wikipedia.org/wiki/Packing_list",
            "https://www.wikivoyage.org/",
        ],
        "Food/Cooking": [
            "https://en.wikipedia.org/wiki/Meal_preparation",
            "https://en.wikipedia.org/wiki/Food_safety",
            "https://www.fda.gov/food",
        ],
        "Health/Fitness": [
            "https://en.wikipedia.org/wiki/Physical_fitness",
            "https://en.wikipedia.org/wiki/Sleep_hygiene",
            "https://www.who.int/health-topics/physical-activity",
        ],
        "Study/Learning": [
            "https://en.wikipedia.org/wiki/Spaced_repetition",
            "https://en.wikipedia.org/wiki/Testing_effect",
            "https://en.wikipedia.org/wiki/Study_skills",
        ],
        "Money/Personal Finance": [
            "https://en.wikipedia.org/wiki/Personal_finance",
            "https://en.wikipedia.org/wiki/Budget",
            "https://en.wikipedia.org/wiki/Interest",
        ],
        "Career/Work": [
            "https://en.wikipedia.org/wiki/Curriculum_vitae",
            "https://en.wikipedia.org/wiki/Job_interview",
            "https://en.wikipedia.org/wiki/Cover_letter",
        ],
        "Relationships/Communication": [
            "https://en.wikipedia.org/wiki/Interpersonal_communication",
            "https://en.wikipedia.org/wiki/Active_listening",
            "https://en.wikipedia.org/wiki/Nonviolent_communication",
        ],
        "Home/Life Admin": [
            "https://en.wikipedia.org/wiki/Checklist",
            "https://en.wikipedia.org/wiki/Time_management",
            "https://en.wikipedia.org/wiki/Personal_organizer",
        ],
        "Shopping/Products": [
            "https://en.wikipedia.org/wiki/Comparison_shopping",
            "https://en.wikipedia.org/wiki/Product_lifecycle",
            "https://en.wikipedia.org/wiki/Warranty",
        ],
        "Events/Leisure": [
            "https://en.wikipedia.org/wiki/Event_planning",
            "https://en.wikipedia.org/wiki/Ticket_(admission)",
            "https://en.wikipedia.org/wiki/Leisure",
        ],
        # tech-ish fallbacks
        "Dev/Tools": [
            "https://en.wikipedia.org/wiki/Debugging",
            "https://en.wikipedia.org/wiki/Software_bug",
            "https://docs.python.org/3/tutorial/errors.html",
        ],
        "Marketing/Social": [
            "https://en.wikipedia.org/wiki/Search_engine_optimization",
            "https://en.wikipedia.org/wiki/Digital_marketing",
            "https://en.wikipedia.org/wiki/Social_media",
        ],
        "Business/Accounting/Tax": [
            "https://en.wikipedia.org/wiki/Accounting",
            "https://en.wikipedia.org/wiki/Tax",
            "https://en.wikipedia.org/wiki/Invoice",
        ],
        "Images/Design": [
            "https://en.wikipedia.org/wiki/Raster_graphics",
            "https://en.wikipedia.org/wiki/Vector_graphics",
            "https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Image_types",
        ],
        "Education/Language": [
            "https://en.wikipedia.org/wiki/Second-language_acquisition",
            "https://en.wikipedia.org/wiki/Language_learning",
            "https://en.wikipedia.org/wiki/Flashcard",
        ],
    }

    default = [
        "https://en.wikipedia.org/wiki/Troubleshooting",
        "https://en.wikipedia.org/wiki/Checklist",
        "https://developer.mozilla.org/",
    ]
    return (base.get(category) or default)




def pick_reference_urls(theme: Theme) -> List[str]:
    """
    References: 10-20 ‚Äúsource-like‚Äù URLs.
    Since we‚Äôre not doing web scraping here, we use:
      - representative post URLs (up to 8)
      - plus supplemental resources and well-known docs
    """
    refs = [p.url for p in theme.representative_posts if p.url]
    refs = uniq_keep_order(refs)
    refs = refs[:8]

    supp = supplemental_resources_for_category(theme.category)
    # mix-in to reach REF_URL_MIN
    extras = [
        "https://support.google.com/webmasters/answer/156184",
        "https://developers.google.com/search/docs/crawling-indexing/sitemaps/overview",
        "https://developer.mozilla.org/en-US/docs/Web/SEO",
        "https://developers.google.com/search/docs/crawling-indexing/robots/intro",
    ]
    pool = uniq_keep_order(supp + extras)
    random.shuffle(pool)

    for u in pool:
        if len(refs) >= REF_URL_MIN:
            break
        if u not in refs:
            refs.append(u)

    # cap
    return refs[: clamp(REF_URL_MAX, REF_URL_MIN, 30)]


def generate_long_article_ja(theme: Theme) -> str:
    """
    Must be >= MIN_ARTICLE_CHARS_JA chars.
    Deterministic long form to guarantee volume without OpenAI.
    """
    intro = (
        f"„Åì„ÅÆ„Éö„Éº„Ç∏„ÅØ„Äå{theme.category}„Äç„Åß„Çà„ÅèËµ∑„Åç„ÇãÊÇ©„Åø„Çí„ÄÅ"
        f"Áü≠ÊôÇÈñì„ÅßÂÆâÂÖ®„Å´Êï¥ÁêÜ„Åó„Å¶Ëß£Ê±∫„Å∏ÈÄ≤„ÇÅ„Çã„Åü„ÇÅ„ÅÆ„Ç¨„Ç§„Éâ„Åß„Åô„ÄÇ\n"
        "„Éù„Ç§„É≥„Éà„ÅØ‚ÄúÊé®Ê∏¨„ÅßÊ±∫„ÇÅÊâì„Å°„Åó„Å™„ÅÑ‚Äù„Åì„Å®„ÄÇÂÜçÁèæÊù°‰ª∂„ÇíÂõ∫ÂÆö„Åó„ÄÅ"
        "ÂΩ±ÈüøÁØÑÂõ≤„ÅåÂ∞è„Åï„ÅÑÈ†Ü„Å´„ÉÅ„Çß„ÉÉ„ÇØ„Åô„Çã„Å†„Åë„Åß„ÄÅÁÑ°ÈßÑ„Å™Ë©¶Ë°åÂõûÊï∞„ÅåÂ§ß„Åç„ÅèÊ∏õ„Çä„Åæ„Åô„ÄÇ\n"
    )
    why = (
        "Â§ö„Åè„ÅÆ„Éà„É©„Éñ„É´„ÅØ„ÄÅ(1)Ë®≠ÂÆö„ÅÆ‰∏ç‰∏ÄËá¥„ÄÅ(2)Ê®©Èôê„ÇÑÊúüÈôê„ÄÅ(3)„Ç≠„É£„ÉÉ„Ç∑„É•/ÂèçÊò†ÂæÖ„Å°„ÄÅ"
        "(4)ÂÖ•ÂäõÊù°‰ª∂„ÅÆÊè∫„Çå„ÄÅ„ÅÆ„Å©„Çå„Åã„Å´ËêΩ„Å°„Åæ„Åô„ÄÇ\n"
        "„Å§„Åæ„Çä„ÄÅ„Åì„ÅÆ4ÁÇπ„ÇíÈ†Ü„Å´ÊΩ∞„Åô„Å†„Åë„Åß‚ÄúÁõ¥„Çâ„Å™„ÅÑÁêÜÁî±‚Äù„ÅÆÂ§ßÂçä„ÅØË™¨Êòé„Åß„Åç„Åæ„Åô„ÄÇ\n"
    )
    detail = (
        "Â§ß‰∫ã„Å™„ÅÆ„ÅØ„ÄåÊúÄÂ∞èÂ§âÊõ¥„Äç„Åß„Åô„ÄÇ‰∏ÄÂ∫¶„Å´Ë§áÊï∞ÁÆáÊâÄ„Çí„ÅÑ„Åò„Çã„Å®„ÄÅÁõ¥„Å£„Åü„Å®„Åó„Å¶„ÇÇÂéüÂõ†„ÅåÂàÜ„Åã„Çâ„ÅöÂÜçÁô∫„Åó„Åæ„Åô„ÄÇ\n"
        "ÊúÄÂ∞èÂ§âÊõ¥‚ÜíÊ§úË®º‚ÜíË®òÈå≤„ÄÅ„ÇíÂÆà„Çã„Å®„ÄÅÊ¨°Âõû„ÅØ„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà„Å†„Åë„ÅßÂæ©Êóß„Åß„Åç„Åæ„Åô„ÄÇ\n"
    )

    examples = "„Äê„Åì„ÅÆ„Éö„Éº„Ç∏„ÅßÊâ±„ÅÜÊÇ©„Åø‰∏ÄË¶ßÔºà‰æãÔºâ„Äë\n" + "\n".join([f"- {p}" for p in theme.problem_list]) + "\n"
    causes = "„ÄêÂéüÂõ†„ÅÆ„Éë„Çø„Éº„É≥ÂàÜ„Åë„Äë\n" + "\n".join([f"- {c}" for c in build_causes(theme.category)]) + "\n"
    steps = "„ÄêÊâãÈ†ÜÔºà„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„ÉàÔºâ„Äë\n" + "\n".join([f"- {s}" for s in build_steps(theme.category)]) + "\n"
    pitfalls = "„Äê„Çà„Åè„ÅÇ„ÇãÂ§±Êïó„Å®ÂõûÈÅøÁ≠ñ„Äë\n" + "\n".join([f"- {p}" for p in build_pitfalls(theme.category)]) + "\n"
    nxt = "„ÄêÁõ¥„Çâ„Å™„ÅÑÂ†¥Âêà„ÅÆÊ¨°„ÅÆÊâã„Äë\n" + "\n".join([f"- {n}" for n in build_next_actions(theme.category)]) + "\n"

    verify = (
        "„ÄêÊ§úË®º„ÅÆ„Ç≥„ÉÑ„Äë\n"
        "- ‚ÄúÊúüÂæÖÁµêÊûú‚Äù„Çí1Êñá„Å´„Åô„ÇãÔºà‰Ωï„Åå„Åß„Åç„Çå„Å∞ÊàêÂäü„ÅãÔºâ\n"
        "- Â§±Êïó„ÅåÂá∫„Åü„Çâ„ÄÅÂÖ•Âäõ„ÉªÁí∞Â¢É„ÉªÊôÇÂàª„Éª„É≠„Ç∞„Çí„Çª„ÉÉ„Éà„ÅßÊÆã„Åô\n"
        "- Áõ¥„Å£„ÅüÁû¨Èñì„Å´„ÄÅ‰Ωï„ÇíÂ§â„Åà„Åü„Åã„Çí1Ë°å„ÅßÊõ∏„Åë„ÇãÁä∂ÊÖã„Å´„Åô„Çã\n"
        "- ÂÜçÁô∫Èò≤Ê≠¢„ÅØ‚ÄúÊ¨°Âõû3ÂàÜ„ÅßÂæ©Êóß„Åß„Åç„Çã„Åã‚Äù„ÅßÂà§Êñ≠„Åô„Çã\n"
        "„Åì„Çå„Å†„Åë„Åß„ÄÅË™øÊüª„ÅåÊÑüÊÉÖ„Åß„ÅØ„Å™„ÅèÊâãÈ†Ü„Å´„Å™„Çä„Åæ„Åô„ÄÇ\n"
    )

    tree = (
        "„ÄêÂàá„ÇäÂàÜ„Åë„ÅÆÂàÜÂ≤êÔºàËø∑„Å£„ÅüÊôÇÁî®Ôºâ„Äë\n"
        "1) Âà•„Éñ„É©„Ç¶„Ç∂/Âà•Á´ØÊú´„Åß„ÇÇÂêå„ÅòÔºü\n"
        "  - „ÅØ„ÅÑ ‚Üí „Çµ„Éº„Éì„Çπ/Ë®≠ÂÆö/Ê®©ÈôêÂÅ¥„ÅåÊøÉÂéö\n"
        "  - „ÅÑ„ÅÑ„Åà ‚Üí „Ç≠„É£„ÉÉ„Ç∑„É•/Êã°ÂºµÊ©üËÉΩ/Á´ØÊú´‰æùÂ≠ò„ÅåÊøÉÂéö\n"
        "2) Âêå„ÅòÂÖ•Âäõ„ÉªÂêå„ÅòÊâãÈ†Ü„ÅßÂÜçÁèæ„Åô„ÇãÔºü\n"
        "  - „ÅØ„ÅÑ ‚Üí ÂéüÂõ†ËøΩË∑°„ÅåÂèØËÉΩ„ÄÇ„É≠„Ç∞„ÇíÂ¢ó„ÇÑ„Åó„Å¶‰∏ÄÁÇπ„Åö„Å§ÊΩ∞„Åô\n"
        "  - „ÅÑ„ÅÑ„Åà ‚Üí ÂÖ•ÂäõÊù°‰ª∂„ÅåÊè∫„Çå„Å¶„ÅÑ„Çã„ÄÇ„Åæ„ÅöÂÜçÁèæÊù°‰ª∂„ÅÆÂõ∫ÂÆö„ÅåÊúÄÂÑ™ÂÖà\n"
        "„Åì„ÅÆÂàÜÂ≤ê„ÇíÂÆà„Çã„Å†„Åë„Åß„ÄÅÁÑ°ÈßÑ„Å™Ë©¶Ë°å„Çí„Åã„Å™„ÇäÊ∏õ„Çâ„Åõ„Åæ„Åô„ÄÇ\n"
    )

    body = "\n".join([intro, why, detail, examples, causes, steps, pitfalls, nxt, verify, tree]).strip()

    # pad to guarantee chars
    if len(body) < MIN_ARTICLE_CHARS_JA:
        pads: List[str] = []
        while len(body) + sum(len(x) for x in pads) < MIN_ARTICLE_CHARS_JA + 200:
            pads.append(
                "„ÄêËøΩÂä†„É°„É¢„Äë\n"
                "ÂïèÈ°å„ÅåË§áÈõë„Å´Ë¶ã„Åà„ÇãÊôÇ„Åª„Å©„ÄÅÊúÄÂàù„Å´‚ÄúÂ§â„Åà„ÅüÁÇπ‚Äù„ÇíÂàóÊåô„Åó„ÄÅ„Åù„Çå„Çí‰∏Ä„Å§„Åö„Å§Êàª„Åó„Å¶Â∑ÆÂàÜ„ÇíÂèñ„Çã„Å®Âæ©Êóß„ÅåÊó©„Åè„Å™„Çä„Åæ„Åô„ÄÇ\n"
                "„É≠„Ç∞„Åå„Å™„ÅÑÂ†¥Âêà„ÅØ„ÄÅ„Åæ„Åö„É≠„Ç∞„Çí‰Ωú„Çã„Åì„Å®„ÅåÊúÄÁü≠„É´„Éº„Éà„Åß„Åô„ÄÇ\n"
            )
        body = body + "\n" + "\n".join(pads)

    return body.strip()


def short_value_line(category: str) -> str:
    """
    One-line value (for Bluesky post draft).
    Keep it short, concrete, non-spammy.
    """
    mapping = {
        "Travel/Planning": "Build a clean itinerary + packing checklist in seconds.",
        "Food/Cooking": "Generate a meal-prep plan + shopping list in seconds.",
        "Health/Fitness": "Turn your goal into a tiny daily routine + tracker in seconds.",
        "Study/Learning": "Generate a study plan + spaced-review schedule in seconds.",
        "Money/Personal Finance": "Make a simple budget + fee checklist in seconds.",
        "Career/Work": "Turn your notes into resume bullets + interview prompts in seconds.",
        "Relationships/Communication": "Get short conversation templates (ask/decline/follow-up) in seconds.",
        "Home/Life Admin": "Create a moving/life-admin checklist in seconds.",
        "Shopping/Products": "Compare options using 3 criteria + decide fast in seconds.",
        "Events/Leisure": "Pick a weekend plan (A/B for weather) in seconds.",
        "Web/Hosting": "Get a DNS/SSL checklist + quick tests in seconds.",
        "PDF/Docs": "Get a PDF convert/merge checklist in seconds.",
        "Media": "Get video compression settings + checklist in seconds.",
        "Data/Spreadsheets": "Get spreadsheet debugging steps + checklist in seconds.",
        "Security/Privacy": "Get privacy/login troubleshooting checklist in seconds.",
        "AI/Automation": "Get automation workflow debugging checklist in seconds.",
    }
    return mapping.get(category, "Get a clean checklist + next steps in seconds.")


# =============================================================================
# Tool UI generation (category-aware planners)
# =============================================================================
def build_tool_ui(theme: Theme) -> str:
    """
    In-page "tool" block (simple + fast):
      - Problems this tool can help solve
      - A lightweight checklist
      - Minimal JS (none) to keep PageSpeed high
    """
    title_raw = getattr(theme, "search_title", None) or getattr(theme, "title", None) or "Tool"
    cat_raw = getattr(theme, "category", None) or "Dev/Tools"
    page_title = html.escape(str(title_raw))
    cat = html.escape(str(cat_raw))

    problems = getattr(theme, "problem_list", None) or []
    if not isinstance(problems, list):
        problems = [problems]

    problems_html_items: List[str] = []
    for p in problems:
        if p is None:
            continue
        s = str(p).strip()
        if not s:
            continue
        problems_html_items.append(f"<li class='leading-relaxed'>{html.escape(s)}</li>")
    if not problems_html_items:
        problems_html_items = ["<li class='text-slate-500'>(no items)</li>"]
    problems_html = "\n".join(problems_html_items)

    # Checklist steps (keep short; long explanation is in the article section)
    steps: List[str] = []
    if "build_steps" in globals():
        try:
            steps = build_steps(getattr(theme, "category", "")) or []
        except Exception:
            # build_steps is optional; keep UI usable
            steps = []
    if not steps:
        steps = [
            "Reproduce the issue with the same inputs",
            "Collect exact error messages + timestamps",
            "Try the smallest safe change first",
            "Verify the fix and document what changed",
        ]

    steps_html = "\n".join(
        f"<li class='leading-relaxed'>{html.escape(str(s))}</li>" for s in steps if str(s).strip()
    ) or "<li class='text-slate-500'>(no steps)</li>"

    return f"""
<section class="rounded-2xl border border-slate-200 bg-white/80 backdrop-blur p-5 shadow-sm dark:border-white/10 dark:bg-white/5">
  <div class="flex items-start justify-between gap-4">
    <div>
      <div class="text-xs uppercase tracking-wider text-slate-500 dark:text-white/50">{cat}</div>
      <h2 class="mt-1 text-lg font-semibold text-slate-900 dark:text-white" data-i18n="tool">Tool</h2>
      <p class="mt-1 text-sm text-slate-600 dark:text-white/70">{page_title}</p>
    </div>
  </div>

  <div class="mt-5 grid gap-4 md:grid-cols-2">
    <div class="rounded-xl border border-slate-200 bg-white/70 p-4 dark:border-white/10 dark:bg-black/20">
      <div class="text-sm font-semibold text-slate-900 dark:text-white" data-i18n="problems">Problems this tool can help solve</div>
      <ul class="mt-2 list-disc pl-5 text-sm text-slate-700 dark:text-white/75">
        {problems_html}
      </ul>
    </div>

    <div class="rounded-xl border border-slate-200 bg-white/70 p-4 dark:border-white/10 dark:bg-black/20">
      <div class="text-sm font-semibold text-slate-900 dark:text-white" data-i18n="quick_answer">Quick checklist</div>
      <ol class="mt-2 list-decimal pl-5 text-sm text-slate-700 dark:text-white/75">
        {steps_html}
      </ol>
    </div>
  </div>
</section>
""".strip()

def render_affiliate_block(affiliate: Dict[str, Any]) -> str:
    if affiliate.get("html"):
        return str(affiliate["html"])
    if affiliate.get("url"):
        title = html_escape(affiliate.get("title", "Recommended"))
        url = html_escape(affiliate["url"])
        return f'<a class="underline" href="{url}" rel="nofollow noopener" target="_blank">{title}</a>'
    return ""


def fetch_unsplash_bg_url() -> str:
    """
    Optional. If UNSPLASH_ACCESS_KEY is set, try to fetch a single abstract gradient image.
    Fallback to empty string (CSS gradients used).
    """
    if not UNSPLASH_ACCESS_KEY:
        return ""
    # Use Unsplash "random" endpoint (no heavy parsing needed)
    # https://api.unsplash.com/photos/random?query=abstract%20gradient&orientation=landscape
    url = "https://api.unsplash.com/photos/random?" + urlencode({
        "query": "abstract gradient",
        "orientation": "landscape",
        "content_filter": "high",
    })
    st, body = http_get(url, headers={"Accept": "application/json", "Authorization": f"Client-ID {UNSPLASH_ACCESS_KEY}"}, timeout=20)
    if st != 200:
        return ""
    try:
        js = json.loads(body)
        u = ((js.get("urls") or {}).get("regular") or "").strip()
        return u
    except Exception:
        return ""


def build_page_html(
    theme: Theme,
    tool_url: str,
    short_url: str,
    affiliates_top2: List[Dict[str, Any]],
    references: List[str],
    supplements: List[str],
    article_ja: str,
    faq: List[Tuple[str, str]],
    related_tools: List[Dict[str, Any]],
    popular_sites: List[Dict[str, Any]],
    hero_bg_url: str = "",
) -> str:
    problems_html = "\n".join([f"<li class='py-1'>{html_escape(p)}</li>" for p in theme.problem_list])

    quick_answer = build_quick_answer(theme.category, theme.keywords)
    causes = build_causes(theme.category)
    steps = build_steps(theme.category)
    pitfalls = build_pitfalls(theme.category)
    next_actions = build_next_actions(theme.category)

    causes_html = "\n".join([f"<li class='py-1'>{html_escape(c)}</li>" for c in causes])
    steps_html = "\n".join([f"<li class='py-1'>{html_escape(s)}</li>" for s in steps])
    pitfalls_html = "\n".join([f"<li class='py-1'>{html_escape(p)}</li>" for p in pitfalls])
    next_html = "\n".join([f"<li class='py-1'>{html_escape(n)}</li>" for n in next_actions])

    faq_html = "\n".join([
        f"""
        <details class="rounded-2xl border border-slate-200/70 dark:border-white/10 bg-white/70 dark:bg-white/5 p-4">
          <summary class="cursor-pointer font-medium">{html_escape(q)}</summary>
          <div class="mt-2 text-slate-900 dark:text-slate-700 dark:text-white/80 leading-relaxed">{html_escape(a)}</div>
        </details>
        """.strip()
        for q, a in faq
    ])

    ref_html = "\n".join([f"<li class='py-1'><a class='underline break-all' href='{html_escape(u)}' target='_blank' rel='noopener'>{html_escape(u)}</a></li>" for u in references])
    sup_html = "\n".join([f"<li class='py-1'><a class='underline break-all' href='{html_escape(u)}' target='_blank' rel='noopener'>{html_escape(u)}</a></li>" for u in supplements])

    # affiliates slot: top2
    aff_blocks = []
    for a in affiliates_top2[:2]:
        title = html_escape(a.get("title", "Recommended"))
        block = render_affiliate_block(a)
        if not block:
            continue
        aff_blocks.append(f"""
        <div class="rounded-2xl border border-slate-200/70 dark:border-white/10 bg-white/70 dark:bg-white/5 p-4">
          <div class="text-sm text-slate-900 dark:text-slate-600 dark:text-white/70 mb-2">{title}</div>
          <div class="prose prose-invert max-w-none">{block}</div>
        </div>
        """.strip())
    if not aff_blocks:
        aff_blocks = ["""
        <div class="rounded-2xl border border-slate-200/70 dark:border-white/10 bg-white/70 dark:bg-white/5 p-4">
          <div class="text-sm text-slate-900 dark:text-slate-600 dark:text-white/70 mb-2">Recommended</div>
          <div class="text-slate-900 dark:text-slate-600 dark:text-white/70">No affiliate available for this category.</div>
        </div>
        """.strip()]
    aff_html = "\n".join(aff_blocks)

    related_html = "\n".join([
        f"<li class='py-1'><a class='underline' href='{html_escape(t.get('url','#'))}'>{html_escape(t.get('title','Tool'))}</a> "
        f"<span class='text-slate-900 dark:text-slate-500 dark:text-white/50 text-xs'>({html_escape(t.get('category',''))})</span></li>"
        for t in related_tools
    ])

    popular_html = "\n".join([
        f"<li class='py-1'><a class='underline' href='{html_escape(t.get('url','#'))}'>{html_escape(t.get('title','Tool'))}</a> "
        f"<span class='text-slate-900 dark:text-slate-500 dark:text-white/50 text-xs'>({html_escape(t.get('category',''))})</span></li>"
        for t in popular_sites
    ])

    canonical = tool_url if tool_url.startswith("http") else (SITE_DOMAIN.rstrip("/") + "/" + theme.slug + "/")

    article_html = "<p class='leading-relaxed whitespace-pre-wrap text-slate-900 dark:text-white/85'>" + html_escape(article_ja) + "</p>"
    try:
        tool_ui = build_tool_ui(theme)
    except Exception:
        logging.exception("build_tool_ui failed")
        raise
    # internal linking: ALWAYS provide a path back to /hub/
    hub_url = SITE_DOMAIN.rstrip("/") + "/hub/"

    # short URL block (for click-through + share)
    share_script = """
<script>
function copyTextFrom(id, btnId){
  const el = document.getElementById(id);
  if(!el) return;
  navigator.clipboard.writeText(el.value).then(()=>{
    const b = document.getElementById(btnId);
    if(b){
      b.textContent = (window.I18N && I18N[document.documentElement.lang] && I18N[document.documentElement.lang].copied) || "Copied";
    }
    setTimeout(()=>{
      const b2 = document.getElementById(btnId);
      if(b2){
        b2.textContent = (window.I18N && I18N[document.documentElement.lang] && I18N[document.documentElement.lang].copy) || "Copy";
      }
    }, 1200);
  });
}
</script>
""".strip()

    bg_css = ""
    if hero_bg_url:
        bg_css = f"""
  <div class="pointer-events-none fixed inset-0 opacity-40">
    <div class="absolute inset-0 bg-cover bg-center" style="background-image:url('{html_escape(hero_bg_url)}')"></div>
    <div class="absolute inset-0 bg-slate-50 dark:bg-zinc-950/70"></div>
  </div>
        """.strip()
    else:
        bg_css = """
  <div class="pointer-events-none fixed inset-0 opacity-80 dark:opacity-70">
    <div class="absolute -top-24 -left-24 h-96 w-96 rounded-full bg-gradient-to-br from-indigo-500/35 to-cyan-400/20 blur-3xl"></div>
    <div class="absolute top-40 -right-24 h-96 w-96 rounded-full bg-gradient-to-br from-emerald-500/25 to-lime-400/10 blur-3xl"></div>
    <div class="absolute bottom-0 left-1/4 h-96 w-96 rounded-full bg-gradient-to-br from-fuchsia-500/20 to-rose-400/10 blur-3xl"></div>
  </div>
        """.strip()

    html_doc = f"""<!doctype html>
<html lang="{html_escape(DEFAULT_LANG)}">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>{html_escape(theme.search_title)} | {html_escape(SITE_BRAND)}</title>
  <meta name="description" content="{html_escape('One-page fix guide + checklist + tool: ' + theme.search_title)}">
  <link rel="canonical" href="{html_escape(canonical)}">
  <meta property="og:title" content="{html_escape(theme.search_title)}">
  <meta property="og:description" content="{html_escape('Fix guide + checklist + FAQ + references')}">
  <meta property="og:type" content="website">
  <meta property="og:url" content="{html_escape(canonical)}">
  <meta name="twitter:card" content="summary_large_image">
  <script>tailwind = window.tailwind || {{}}; tailwind.config = {{ darkMode: "class" }};</script>
  <script src="https://cdn.tailwindcss.com"></script>
  <style>
    :root {{ color-scheme: dark; }}
    body {{
      font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto,
        "Noto Sans JP","Noto Sans KR","Noto Sans SC", Arial, "Apple Color Emoji","Segoe UI Emoji";
    }}
    .glass {{ backdrop-filter: blur(10px); }}
  </style>
</head>
<body class="min-h-screen bg-slate-50 dark:bg-zinc-950 text-slate-900 dark:text-white">
  {bg_css}

  <header class="relative z-10 mx-auto max-w-6xl px-4 py-6">
    <div class="flex items-center justify-between gap-4">
      <a href="{html_escape(hub_url)}" class="flex items-center gap-3">
        <div class="h-10 w-10 rounded-2xl bg-slate-100/80 dark:bg-white/10 border border-slate-200/70 dark:border-white/10 flex items-center justify-center font-bold">üçä</div>
        <div>
          <div class="font-semibold leading-tight">{html_escape(SITE_BRAND)}</div>
          <div class="text-xs text-slate-900 dark:text-slate-500 dark:text-white/60">Hub ‚Üí categories / popular / new</div>
        </div>
      </a>

      <nav class="flex items-center gap-3 text-sm">
        <a class="text-slate-900 dark:text-slate-700 dark:text-white/80 hover:text-slate-900 dark:text-white" href="{html_escape(hub_url)}" data-i18n="home">Home</a>
        <a class="text-slate-900 dark:text-slate-700 dark:text-white/80 hover:text-slate-900 dark:text-white" href="{html_escape(hub_url)}#about" data-i18n="about">About Us</a>
        <a class="text-slate-900 dark:text-slate-700 dark:text-white/80 hover:text-slate-900 dark:text-white" href="{html_escape(hub_url)}#tools" data-i18n="all_tools">All Tools</a>
        <button id="themeBtn" type="button" class="ml-2 rounded-xl bg-slate-100/80 hover:bg-slate-200/70 dark:bg-white/10 dark:hover:bg-white/20 border border-slate-200/70 dark:border-white/10 px-2 py-1 text-xs" aria-label="Theme">üåì</button>
        <select id="langSel" class="ml-2 rounded-xl bg-slate-100/80 dark:bg-white/10 border border-slate-200/70 dark:border-white/10 px-2 py-1 text-xs">
          <option value="en">EN</option>
          <option value="ja">JA</option>
          <option value="ko">KO</option>
          <option value="zh">ZH</option>
        </select>
      </nav>
    </div>
  </header>

  <main class="relative z-10 mx-auto max-w-6xl px-4 pb-16">
    <section class="rounded-3xl border border-slate-200/70 dark:border-white/10 bg-white/70 dark:bg-white/5 glass p-6 md:p-8">
      <div class="flex flex-col md:flex-row md:items-start md:justify-between gap-4">
        <div>
          <h1 class="text-2xl md:text-3xl font-semibold leading-tight">{html_escape(theme.search_title)}</h1>
          <p class="mt-2 text-slate-900 dark:text-slate-600 dark:text-white/70">
            Category: <span class="text-slate-900 dark:text-slate-900 dark:text-white/90">{html_escape(theme.category)}</span> ¬∑
            Updated: <span class="text-slate-900 dark:text-slate-900 dark:text-white/90">{html_escape(now_iso())}</span>
          </p>
        </div>
        <div class="rounded-2xl border border-slate-200/70 dark:border-white/10 bg-white/60 dark:bg-black/20 p-4 w-full md:w-[360px]">
          <div class="text-sm text-slate-900 dark:text-slate-600 dark:text-white/70 mb-2" data-i18n="share">Share</div>
          <div class="space-y-2">
            <div class="text-xs text-slate-900 dark:text-slate-500 dark:text-white/60">Short URL (for posts)</div>
            <div class="flex items-center gap-2">
              <input id="shortUrl" value="{html_escape(short_url)}" class="w-full rounded-xl bg-white/80 dark:bg-black/40 border border-slate-200/70 dark:border-white/10 px-3 py-2 text-xs" readonly>
              <button id="copyBtnShort" class="rounded-xl bg-slate-100/80 dark:bg-white/10 border border-slate-200/70 dark:border-white/10 px-3 py-2 text-xs" data-i18n="copy" onclick="copyTextFrom('shortUrl','copyBtnShort')">Copy</button>
            </div>

            <div class="text-xs text-slate-900 dark:text-slate-500 dark:text-white/60">Full URL</div>
            <div class="flex items-center gap-2">
              <input id="fullUrl" value="{html_escape(tool_url)}" class="w-full rounded-xl bg-white/80 dark:bg-black/40 border border-slate-200/70 dark:border-white/10 px-3 py-2 text-xs" readonly>
              <button id="copyBtnFull" class="rounded-xl bg-slate-100/80 dark:bg-white/10 border border-slate-200/70 dark:border-white/10 px-3 py-2 text-xs" data-i18n="copy" onclick="copyTextFrom('fullUrl','copyBtnFull')">Copy</button>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="mt-6">
  {tool_ui}
</section>

<script>
function copyTextFrom(inputId, btnId) {{
  const input = document.getElementById(inputId);
  const btn = document.getElementById(btnId);
  if (!input) return;

  const text = String(input.value ?? "");

  const done = () => {{
    if (!btn) return;
    const prev = btn.textContent;
    btn.textContent = "Copied";
    setTimeout(() => {{ btn.textContent = prev; }}, 1200);
  }};

  if (navigator.clipboard && window.isSecureContext) {{
    navigator.clipboard.writeText(text).then(done).catch(() => {{
      input.focus();
      input.select();
      try {{ document.execCommand("copy"); }} catch (e) {{}}
      done();
    }});
    return;
  }}

  input.focus();
  input.select();
  try {{ document.execCommand("copy"); }} catch (e) {{}}
  done();
}}
</script>


    <section class="mt-6 grid grid-cols-1 lg:grid-cols-3 gap-6">
      <div class="lg:col-span-2 space-y-6">
        <div class="rounded-3xl border border-slate-200/70 dark:border-white/10 bg-white/70 dark:bg-white/5 p-6">
          <h2 class="text-xl font-semibold" data-i18n="problems">Problems this tool can help with</h2>
          <ul class="mt-3 text-slate-900 dark:text-white/85 list-disc list-inside">
            {problems_html}
          </ul>
        </div>

        <div class="rounded-3xl border border-slate-200/70 dark:border-white/10 bg-white/70 dark:bg-white/5 p-6">
          <h2 class="text-xl font-semibold" data-i18n="quick_answer">Quick answer</h2>
          <pre class="mt-3 text-slate-900 dark:text-white/85 whitespace-pre-wrap leading-relaxed">{html_escape(quick_answer)}</pre>
        </div>

        <div class="rounded-3xl border border-slate-200/70 dark:border-white/10 bg-white/70 dark:bg-white/5 p-6">
          <h2 class="text-xl font-semibold" data-i18n="causes">Common causes</h2>
          <ul class="mt-3 text-slate-900 dark:text-white/85 list-disc list-inside">
            {causes_html}
          </ul>
        </div>

        <div class="rounded-3xl border border-slate-200/70 dark:border-white/10 bg-white/70 dark:bg-white/5 p-6">
          <h2 class="text-xl font-semibold" data-i18n="steps">Step-by-step checklist</h2>
          <ul class="mt-3 text-slate-900 dark:text-white/85 list-disc list-inside">
            {steps_html}
          </ul>
        </div>

        <div class="rounded-3xl border border-slate-200/70 dark:border-white/10 bg-white/70 dark:bg-white/5 p-6">
          <h2 class="text-xl font-semibold" data-i18n="pitfalls">Common pitfalls & how to avoid them</h2>
          <ul class="mt-3 text-slate-900 dark:text-white/85 list-disc list-inside">
            {pitfalls_html}
          </ul>
        </div>

        <div class="rounded-3xl border border-slate-200/70 dark:border-white/10 bg-white/70 dark:bg-white/5 p-6">
          <h2 class="text-xl font-semibold" data-i18n="next">If it still doesn‚Äôt work</h2>
          <ul class="mt-3 text-slate-900 dark:text-white/85 list-disc list-inside">
            {next_html}
          </ul>
        </div>

        <div class="rounded-3xl border border-slate-200/70 dark:border-white/10 bg-white/70 dark:bg-white/5 p-6">
          <h2 class="text-xl font-semibold">Long guide (JP, 2500+ chars)</h2>
          <div class="mt-3">{article_html}</div>
        </div>

        <div class="rounded-3xl border border-slate-200/70 dark:border-white/10 bg-white/70 dark:bg-white/5 p-6">
          <h2 class="text-xl font-semibold" data-i18n="faq">FAQ</h2>
          <div class="mt-3 space-y-3">{faq_html}</div>
        </div>

        <div class="rounded-3xl border border-slate-200/70 dark:border-white/10 bg-white/70 dark:bg-white/5 p-6">
          <h2 class="text-xl font-semibold" data-i18n="references">Reference links</h2>
          <ul class="mt-3 text-slate-900 dark:text-white/85 list-disc list-inside">
            {ref_html}
          </ul>
        </div>

        <div class="rounded-3xl border border-slate-200/70 dark:border-white/10 bg-white/70 dark:bg-white/5 p-6">
          <h2 class="text-xl font-semibold" data-i18n="supplement">Supplementary resources</h2>
          <ul class="mt-3 text-slate-900 dark:text-white/85 list-disc list-inside">
            {sup_html}
          </ul>
        </div>
      </div>

      <aside class="space-y-6">
        <div class="rounded-3xl border border-slate-200/70 dark:border-white/10 bg-white/70 dark:bg-white/5 p-6">
          <h3 class="text-lg font-semibold" data-i18n="aff_title">Recommended</h3>
          <div class="mt-3 space-y-3">
            <!-- AFF_SLOT (top2 injected) -->
            {aff_html}
          </div>
        </div>

        <div class="rounded-3xl border border-slate-200/70 dark:border-white/10 bg-white/70 dark:bg-white/5 p-6">
          <h3 class="text-lg font-semibold" data-i18n="related">Related tools</h3>
          <ul class="mt-3 text-slate-900 dark:text-white/85 list-disc list-inside">
            {related_html}
          </ul>
        </div>

        <div class="rounded-3xl border border-slate-200/70 dark:border-white/10 bg-white/70 dark:bg-white/5 p-6">
          <h3 class="text-lg font-semibold" data-i18n="popular">Popular tools</h3>
          <ul class="mt-3 text-slate-900 dark:text-white/85 list-disc list-inside">
            {popular_html}
          </ul>
        </div>
      </aside>
    </section>
  </main>

  <footer class="relative z-10 mt-10 bg-zinc-900/60 border-t border-slate-200/70 dark:border-white/10">
    <div class="mx-auto max-w-6xl px-4 py-10 grid grid-cols-1 md:grid-cols-4 gap-8">
      <div class="md:col-span-2">
        <div class="flex items-center gap-3">
          <div class="h-10 w-10 rounded-2xl bg-slate-100/80 dark:bg-white/10 border border-slate-200/70 dark:border-white/10 flex items-center justify-center font-bold">üçä</div>
          <div>
            <div class="font-semibold">{html_escape(SITE_BRAND)}</div>
            <div class="text-xs text-slate-900 dark:text-slate-500 dark:text-white/60" data-i18n="footer_note">Practical, fast, and respectful guides‚Äîbuilt to reduce wasted trial-and-error.</div>
          </div>
        </div>
        <div class="mt-3 text-xs text-slate-900 dark:text-slate-500 dark:text-white/60">Contact: {html_escape(SITE_CONTACT_EMAIL)}</div>
      </div>

      <div class="text-sm">
        <div class="font-semibold mb-2">Legal</div>
        <ul class="space-y-2 text-slate-900 dark:text-slate-600 dark:text-white/70">
          <li><a class="underline" href="{html_escape(SITE_DOMAIN.rstrip('/') + '/policies/privacy.html')}" data-i18n="privacy">Privacy</a></li>
          <li><a class="underline" href="{html_escape(SITE_DOMAIN.rstrip('/') + '/policies/terms.html')}" data-i18n="terms">Terms</a></li>
          <li><a class="underline" href="{html_escape(SITE_DOMAIN.rstrip('/') + '/policies/contact.html')}" data-i18n="contact">Contact</a></li>
        </ul>
      </div>

      <div class="text-sm">
        <div class="font-semibold mb-2">Hub</div>
        <ul class="space-y-2 text-slate-900 dark:text-slate-600 dark:text-white/70">
          <li><a class="underline" href="{html_escape(hub_url)}">/hub/</a></li>
          <li><a class="underline" href="{html_escape(hub_url)}#tools">All tools</a></li>
        </ul>
      </div>
    </div>
  </footer>

  {build_i18n_script(DEFAULT_LANG)}
  {share_script}
</body>
</html>
"""
    return html_doc


# =============================================================================
# Site building helpers (slug collision safe, related/popular)
# =============================================================================
def allocate_unique_slug(base_slug: str) -> str:
    """
    No-overwrite rule: if goliath/pages/<slug> exists, use -2, -3...
    """
    base = safe_slug(base_slug)
    if not os.path.exists(os.path.join(PAGES_DIR, base)):
        return base
    for i in range(2, 100):
        cand = f"{base}-{i}"
        if not os.path.exists(os.path.join(PAGES_DIR, cand)):
            return cand
    # extremely unlikely
    return f"{base}-{sha1(base)[:6]}"


def site_url_for_slug(slug: str) -> str:
    """
    Public URL for a generated page.
    Your generation path: goliath/pages/<slug>/index.html
    """
    return SITE_DOMAIN.rstrip("/") + f"/goliath/pages/{slug}/"


def choose_related_tools(all_sites: List[Dict[str, Any]], category: str, exclude_slug: str, n: int = 5) -> List[Dict[str, Any]]:
    same = [s for s in all_sites if s.get("category") == category and s.get("slug") != exclude_slug]
    other = [s for s in all_sites if s.get("slug") != exclude_slug]
    random.shuffle(same)
    random.shuffle(other)
    picks = (same + other)[:n]
    return [{"title": s.get("search_title") or s.get("title", "Tool"), "url": s.get("url", "#"), "category": s.get("category", ""), "slug": s.get("slug", "")} for s in picks]


def compute_popular_sites(all_sites: List[Dict[str, Any]], n: int = 6) -> List[Dict[str, Any]]:
    def metric(s: Dict[str, Any]) -> float:
        for k in ["views", "score", "popularity"]:
            if k in s:
                try:
                    return float(s.get(k, 0))
                except Exception:
                    pass
        iso = s.get("updated_at") or s.get("created_at") or ""
        try:
            return dt.datetime.fromisoformat(iso.replace("Z", "+00:00")).timestamp()
        except Exception:
            return 0.0

    sites = list(all_sites)
    sites.sort(key=lambda x: metric(x), reverse=True)
    return [{"title": s.get("search_title") or s.get("title", "Tool"), "url": s.get("url", "#"), "category": s.get("category", ""), "slug": s.get("slug", "")} for s in sites[:n]]


# =============================================================================
# Policies (legal fortress) - /policies/ only (allowed)
# =============================================================================

def ensure_policies() -> List[str]:
    """
    Create/overwrite policies pages (privacy/terms/contact) under /policies/.
    Pages are styled consistently with tool pages (light + dark, i18n switch).
    Returns absolute URLs for sitemap.
    """
    os.makedirs(POLICIES_DIR, exist_ok=True)
    privacy_path = os.path.join(POLICIES_DIR, "privacy.html")
    terms_path = os.path.join(POLICIES_DIR, "terms.html")
    contact_path = os.path.join(POLICIES_DIR, "contact.html")

    hub_url = SITE_DOMAIN.rstrip("/") + "/hub/"
    i18n_json = json.dumps(I18N, ensure_ascii=False)
    langs_json = json.dumps(sorted(list(I18N.keys())), ensure_ascii=False)

    # Full text per language (keep it static & AdSense-friendly)
    POLICY_TEXT = {
        "privacy": {
            "en": [
                "This site uses cookies and similar technologies to improve usability and measure performance.",
                "We may use Google AdSense to display ads. Third-party vendors, including Google, may use cookies to serve ads based on a user‚Äôs prior visits.",
                "You can control cookies in your browser settings. Disabling cookies may affect site features.",
            ],
            "ja": [
                "ÂΩì„Çµ„Ç§„Éà„ÅØÂà©‰æøÊÄßÂêë‰∏ä„ÉªË®àÊ∏¨„ÅÆ„Åü„ÇÅ„ÄÅCookieÁ≠â„ÅÆÊäÄË°ì„Çí‰ΩøÁî®„Åô„ÇãÂ†¥Âêà„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ",
                "ÂΩì„Çµ„Ç§„Éà„Åß„ÅØ Google AdSense „ÇíÂà©Áî®„Åó„Å¶Â∫ÉÂëä„ÇíÈÖç‰ø°„Åô„ÇãÂ†¥Âêà„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇÁ¨¨‰∏âËÄÖÈÖç‰ø°‰∫ãÊ•≠ËÄÖÔºàGoogleÁ≠âÔºâ„Åå Cookie „Çí‰ΩøÁî®„Åó„ÄÅÈÅéÂéª„ÅÆ„Ç¢„ÇØ„Çª„ÇπÊÉÖÂ†±„Å´Âü∫„Å•„ÅÑ„Å¶Â∫ÉÂëä„ÇíË°®Á§∫„Åô„Çã„Åì„Å®„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ",
                "Cookie „ÅØ„Éñ„É©„Ç¶„Ç∂Ë®≠ÂÆö„ÅßÁÑ°ÂäπÂåñ„Åß„Åç„Åæ„Åô„Åå„ÄÅ‰∏ÄÈÉ®Ê©üËÉΩ„ÅåÂà©Áî®„Åß„Åç„Å™„Åè„Å™„ÇãÂ†¥Âêà„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ",
            ],
            "ko": [
                "Ïù¥ ÏÇ¨Ïù¥Ìä∏Îäî ÏÇ¨Ïö©ÏÑ± Í∞úÏÑ† Î∞è ÏÑ±Îä• Ï∏°Ï†ïÏùÑ ÏúÑÌï¥ Ïø†ÌÇ§ Îì± Ïú†ÏÇ¨ Í∏∞Ïà†ÏùÑ ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.",
                "Ïù¥ ÏÇ¨Ïù¥Ìä∏Îäî Google AdSenseÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Í¥ëÍ≥†Î•º Í≤åÏû¨Ìï† Ïàò ÏûàÏäµÎãàÎã§. Google Îì± Ï†ú3Ïûê Í≥µÍ∏âÏóÖÏ≤¥Îäî ÏÇ¨Ïö©ÏûêÏùò Ïù¥Ï†Ñ Î∞©Î¨∏ Ï†ïÎ≥¥Î•º Í∏∞Î∞òÏúºÎ°ú Ïø†ÌÇ§Î•º ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.",
                "Ïø†ÌÇ§Îäî Î∏åÎùºÏö∞Ï†Ä ÏÑ§Ï†ïÏóêÏÑú Í¥ÄÎ¶¨/ÎπÑÌôúÏÑ±ÌôîÌï† Ïàò ÏûàÏúºÎÇò ÏùºÎ∂Ä Í∏∞Îä•Ïù¥ Ï†úÌïúÎê† Ïàò ÏûàÏäµÎãàÎã§.",
            ],
            "zh": [
                "Êú¨ÁΩëÁ´ôÂèØËÉΩ‰ΩøÁî® Cookie Á≠âÊäÄÊúØ‰ª•ÊèêÂçáÂèØÁî®ÊÄßÂπ∂ËøõË°åÊÄßËÉΩÁªüËÆ°„ÄÇ",
                "Êú¨ÁΩëÁ´ôÂèØËÉΩ‰ΩøÁî® Google AdSense ÊäïÊîæÂπøÂëä„ÄÇÂåÖÊã¨ Google Âú®ÂÜÖÁöÑÁ¨¨‰∏âÊñπ‰æõÂ∫îÂïÜÂèØËÉΩ‰ºö‰ΩøÁî® CookieÔºåÊ†πÊçÆÁî®Êà∑‰ª•ÂæÄËÆøÈóÆËÆ∞ÂΩïÊäïÊîæÂπøÂëä„ÄÇ",
                "ÊÇ®ÂèØ‰ª•Âú®ÊµèËßàÂô®ËÆæÁΩÆ‰∏≠ÁÆ°ÁêÜ/Á¶ÅÁî® CookieÔºå‰ΩÜÂèØËÉΩ‰ºöÂΩ±ÂìçÈÉ®ÂàÜÂäüËÉΩ„ÄÇ",
            ],
        },
        "terms": {
            "en": [
                "Use of this site is at your own risk. The information and tools are provided ‚Äúas is‚Äù without warranties.",
                "We are not liable for any loss or damage arising from use of calculations, checklists, or recommendations.",
                "You are responsible for verifying results and complying with applicable laws and service terms.",
            ],
            "ja": [
                "ÂΩì„Çµ„Ç§„Éà„ÅÆÊÉÖÂ†±„Éª„ÉÑ„Éº„É´„ÅØÁèæÁä∂ÊúâÂßø„ÅßÊèê‰æõ„Åï„Çå„Åæ„Åô„ÄÇÂà©Áî®„ÅØËá™Â∑±Ë≤¨‰ªª„Åß„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô„ÄÇ",
                "Ë®àÁÆóÁµêÊûú„Éª„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà„ÉªÊèêÊ°àÂÜÖÂÆπ„ÇíÂà©Áî®„Åó„Åü„Åì„Å®„Å´„Çà„ÇäÁîü„Åò„ÅüÊêçÂÆ≥„Å´„Å§„ÅÑ„Å¶„ÄÅÂΩì„Çµ„Ç§„Éà„ÅØË≤¨‰ªª„ÇíË≤†„ÅÑ„Åæ„Åõ„Çì„ÄÇ",
                "ÊúÄÁµÇÂà§Êñ≠„ÅØ„ÅîËá™Ë∫´„ÅßË°å„ÅÑ„ÄÅÂêÑÁ®ÆË¶èÁ¥Ñ„ÉªÊ≥ïÂæã„ÇíÈÅµÂÆà„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ",
            ],
            "ko": [
                "Î≥∏ ÏÇ¨Ïù¥Ìä∏Ïùò Ï†ïÎ≥¥/ÎèÑÍµ¨Îäî ‚ÄúÏûàÎäî Í∑∏ÎåÄÎ°ú‚Äù Ï†úÍ≥µÎêòÎ©∞ Ïù¥Ïö©ÏùÄ ÏÇ¨Ïö©Ïûê Ï±ÖÏûÑÏûÖÎãàÎã§.",
                "Í≥ÑÏÇ∞ Í≤∞Í≥º, Ï≤¥ÌÅ¨Î¶¨Ïä§Ìä∏, Í∂åÍ≥†ÏÇ¨Ìï≠ ÏÇ¨Ïö©ÏúºÎ°ú Î∞úÏÉùÌïú ÏÜêÌï¥Ïóê ÎåÄÌï¥ ÎãπÏÇ¨Îäî Ï±ÖÏûÑÏùÑ ÏßÄÏßÄ ÏïäÏäµÎãàÎã§.",
                "Í≤∞Í≥ºÎ•º Í≤ÄÏ¶ùÌïòÍ≥† Í¥ÄÎ†® Î≤ï/ÏÑúÎπÑÏä§ ÏïΩÍ¥ÄÏùÑ Ï§ÄÏàòÌï† Ï±ÖÏûÑÏùÄ ÏÇ¨Ïö©ÏûêÏóêÍ≤å ÏûàÏäµÎãàÎã§.",
            ],
            "zh": [
                "Êú¨ÁΩëÁ´ôÁöÑ‰ø°ÊÅØ‰∏éÂ∑•ÂÖ∑Êåâ‚ÄúÁé∞Áä∂‚ÄùÊèê‰æõÔºå‰ΩøÁî®È£éÈô©Áî±ÊÇ®Ëá™Ë°åÊâøÊãÖ„ÄÇ",
                "Âõ†‰ΩøÁî®ËÆ°ÁÆóÁªìÊûú„ÄÅÊ∏ÖÂçïÊàñÂª∫ËÆÆÈÄ†ÊàêÁöÑ‰ªª‰ΩïÊçüÂ§±ÔºåÊú¨ÁΩëÁ´ô‰∏çÊâøÊãÖË¥£‰ªª„ÄÇ",
                "ËØ∑Ëá™Ë°åÊ†∏ÂØπÁªìÊûúÂπ∂ÈÅµÂÆàÁõ∏ÂÖ≥Ê≥ïÂæãÂèäÊúçÂä°Êù°Ê¨æ„ÄÇ",
            ],
        },
        "contact": {
            "en": [
                f"Operator: {SITE_BRAND}",
                f"Contact: {SITE_CONTACT_EMAIL}",
                "For inquiries about ads, content, or corrections, please email us.",
            ],
            "ja": [
                f"ÈÅãÂñ∂ËÄÖ: {SITE_BRAND}",
                f"ÈÄ£Áµ°ÂÖà: {SITE_CONTACT_EMAIL}",
                "Â∫ÉÂëä„ÉªÂÜÖÂÆπ„ÉªË®ÇÊ≠£„ÅÆ„ÅîÈÄ£Áµ°„ÅØ„É°„Éº„É´„Åß„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô„ÄÇ",
            ],
            "ko": [
                f"Ïö¥ÏòÅÏûê: {SITE_BRAND}",
                f"Ïó∞ÎùΩÏ≤ò: {SITE_CONTACT_EMAIL}",
                "Í¥ëÍ≥†/ÏΩòÌÖêÏ∏†/Ï†ïÏ†ï Î¨∏ÏùòÎäî Ïù¥Î©îÏùºÎ°ú Ïó∞ÎùΩÌï¥ Ï£ºÏÑ∏Ïöî.",
            ],
            "zh": [
                f"ËøêËê•ËÄÖ: {SITE_BRAND}",
                f"ËÅîÁ≥ªÊñπÂºè: {SITE_CONTACT_EMAIL}",
                "ÂÖ≥‰∫éÂπøÂëä„ÄÅÂÜÖÂÆπÊàñÊõ¥Ê≠£Á≠âÂí®ËØ¢ËØ∑ÂèëÈÄÅÈÇÆ‰ª∂„ÄÇ",
            ],
        },
    }

    def build_policy_html(page_key: str) -> str:
        title = {"privacy": "privacy", "terms": "terms", "contact": "contact"}[page_key]
        # page titles: use data-i18n
        body_json = json.dumps(POLICY_TEXT[page_key], ensure_ascii=False)

        return f"""<!doctype html>
<html lang="{html_escape(DEFAULT_LANG)}" class="">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>{html_escape(SITE_BRAND)} | {title}</title>
  <script>tailwind = window.tailwind || {{}}; tailwind.config = {{ darkMode: "class" }};</script>
  <script src="https://cdn.tailwindcss.com"></script>
  <style>
    :root {{ color-scheme: light; }}
    html.dark {{ color-scheme: dark; }}
    body {{
      font-family: ui-sans-serif, system-ui, -apple-system, "Inter", Segoe UI, Roboto,
        "Noto Sans JP","Noto Sans KR","Noto Sans SC", Arial, "Apple Color Emoji","Segoe UI Emoji";
    }}
  </style>
</head>
<body class="min-h-screen bg-slate-50 dark:bg-zinc-950 text-slate-900 dark:text-white">
  <header class="border-b border-slate-200/70 dark:border-white/10">
    <div class="mx-auto max-w-5xl px-4 py-4 flex items-center justify-between">
      <a href="{html_escape(hub_url)}" class="flex items-center gap-2">
        <span class="text-lg">{html_escape(SITE_LOGO)}</span>
        <span class="font-semibold">{html_escape(SITE_BRAND)}</span>
      </a>
      <nav class="flex items-center gap-4 text-sm">
        <a class="text-slate-700 hover:text-slate-900 dark:text-white/80 dark:hover:text-white" href="{html_escape(hub_url)}" data-i18n="home">Home</a>
        <a class="text-slate-700 hover:text-slate-900 dark:text-white/80 dark:hover:text-white" href="{html_escape(hub_url)}#about" data-i18n="about">About Us</a>
        <a class="text-slate-700 hover:text-slate-900 dark:text-white/80 dark:hover:text-white" href="{html_escape(hub_url)}#tools" data-i18n="all_tools">All Tools</a>
        <button id="themeBtn" type="button" class="ml-2 rounded-xl bg-slate-100/80 hover:bg-slate-200/70 dark:bg-white/10 dark:hover:bg-white/20 border border-slate-200/70 dark:border-white/10 px-2 py-1 text-xs" aria-label="Theme">üåì</button>
        <select id="langSel" class="ml-2 rounded-xl bg-slate-100/80 dark:bg-white/10 border border-slate-200/70 dark:border-white/10 px-2 py-1 text-xs">
          <option value="en">EN</option>
          <option value="ja">JA</option>
          <option value="ko">KO</option>
          <option value="zh">ZH</option>
        </select>
      </nav>
    </div>
  </header>

  <main class="mx-auto max-w-3xl px-4 py-10">
    <h1 class="text-2xl font-semibold" data-i18n="{title}">{title.capitalize()}</h1>
    <div id="policyBody" class="mt-6 space-y-3 text-slate-700 dark:text-white/70 leading-relaxed"></div>
  </main>

  <footer class="border-t border-slate-200/70 dark:border-white/10">
    <div class="mx-auto max-w-5xl px-4 py-8 text-xs text-slate-500 dark:text-white/60">
      <div>{html_escape(SITE_BRAND)} ¬∑ <span data-i18n="footer_note">Practical, fast, and respectful guides‚Äîbuilt to reduce wasted trial-and-error.</span></div>
    </div>
  </footer>

  <script>
  const I18N = {i18n_json};
  const LANGS = {langs_json};
  const BODY = {body_json};

  function t(lang, key) {{
    return (I18N[lang] && I18N[lang][key]) || (I18N["{DEFAULT_LANG}"][key]) || key;
  }}

  function setLang(lang) {{
    if (!LANGS.includes(lang)) lang = "{DEFAULT_LANG}";
    document.documentElement.setAttribute("lang", lang);
    localStorage.setItem("lang", lang);

    document.querySelectorAll("[data-i18n]").forEach(el => {{
      const key = el.getAttribute("data-i18n");
      el.textContent = t(lang, key);
    }});

    const container = document.getElementById("policyBody");
    if (container) {{
      const ps = (BODY[lang] || BODY["{DEFAULT_LANG}"] || []);
      container.innerHTML = ps.map(p => `<p>${{p}}</p>`).join("");
    }}
  }}

  function setTheme(mode) {{
    if (mode === "dark") document.documentElement.classList.add("dark");
    else document.documentElement.classList.remove("dark");
    localStorage.setItem("theme", mode);
  }}

  function init() {{
    const savedTheme = localStorage.getItem("theme");
    const prefersDark = window.matchMedia && window.matchMedia("(prefers-color-scheme: dark)").matches;
    setTheme(savedTheme || (prefersDark ? "dark" : "light"));

    const savedLang = localStorage.getItem("lang") || "{DEFAULT_LANG}";
    setLang(savedLang);

    const sel = document.getElementById("langSel");
    if (sel) {{
      sel.value = savedLang;
      sel.addEventListener("change", (e) => setLang(e.target.value));
    }}

    const btn = document.getElementById("themeBtn");
    if (btn) {{
      btn.addEventListener("click", () => {{
        const isDark = document.documentElement.classList.contains("dark");
        setTheme(isDark ? "light" : "dark");
      }});
    }}
  }}

  document.addEventListener("DOMContentLoaded", init);
  </script>
</body>
</html>"""

    privacy = build_policy_html("privacy")
    terms = build_policy_html("terms")
    contact = build_policy_html("contact")

    write_text(privacy_path, privacy)
    write_text(terms_path, terms)
    write_text(contact_path, contact)

    return [
        SITE_DOMAIN.rstrip("/") + "/policies/privacy.html",
        SITE_DOMAIN.rstrip("/") + "/policies/terms.html",
        SITE_DOMAIN.rstrip("/") + "/policies/contact.html",
    ]


# =============================================================================
# Sitemap + robots + ping
# =============================================================================
def build_sitemap(urls: List[str]) -> str:
    urls = uniq_keep_order([u for u in urls if isinstance(u, str) and u.startswith("http")])
    lastmod = dt.datetime.now(dt.timezone.utc).date().isoformat()
    items = []
    for u in urls:
        items.append(
            f"<url><loc>{html_escape(u)}</loc><lastmod>{lastmod}</lastmod></url>"
        )
    xml = f"""<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
{''.join(items)}
</urlset>
"""
    return xml


def build_robots(sitemap_url: str) -> str:
    return f"""User-agent: *
Allow: /

Sitemap: {sitemap_url}
"""


def ping_search_engines(sitemap_url: str) -> None:
    """
    Optional ping. Not guaranteed, but logs status.
    """
    targets = [
        "https://www.google.com/ping?" + urlencode({"sitemap": sitemap_url}),
        "https://www.bing.com/ping?" + urlencode({"sitemap": sitemap_url}),
    ]
    for u in targets:
        st, body = http_get(u, headers={"User-Agent": "goliath-tool/1.0"}, timeout=20)
        logging.info("Ping sitemap: %s -> %s", u, st)


# =============================================================================
# Validation + Auto-fix (up to MAX_AUTOFIX)
# =============================================================================
REQUIRED_MARKERS = [
    "AFF_SLOT",
    "Long guide (JP, 2500+ chars)",
    "Reference links",
    "<script src=\"https://cdn.tailwindcss.com\"></script>",
]

def validate_site_html(html_text: str) -> List[str]:
    errs: List[str] = []
    if not html_text or len(html_text) < 2000:
        errs.append("html_too_short")
        return errs
    for m in REQUIRED_MARKERS:
        if m not in html_text:
            errs.append(f"missing:{m}")
    # article length check: crude
    if "Long guide (JP" in html_text:
        # ensure article content roughly long
        if html_text.count("„Äê") < 6 and len(html_text) < 12000:
            errs.append("article_maybe_too_short")
    return errs


# =============================================================================
# Reply generation (EN, short, no ‚ÄúAI/bot‚Äù words, URL last line)
# =============================================================================
FORBIDDEN_REPLY_WORDS = ["ai", "bot", "automation", "automated"]

def openai_generate_reply_stub(post: Post, tool_url: str) -> str:
    """
    Deterministic reply. 280-400 chars target. English. Last line is URL only.
    """
    # empathy first
    t = post.norm_text()
    # short summary (very light)
    summary = "That sounds frustrating‚Äîespecially when you‚Äôre trying to decide quickly."
    if any(w in t.lower() for w in ["overwhelmed", "confused", "stuck", "don‚Äôt know"]):
        summary = "That sounds really overwhelming‚Äîespecially when you‚Äôre stuck and need a clear next step."
    elif any(w in t.lower() for w in ["today", "tomorrow", "this week", "urgent", "deadline"]):
        summary = "That‚Äôs stressful‚Äîespecially with the clock ticking."

    line2 = "I put together a simple one-page guide + checklist that should help you move forward:"
    reply = f"{summary}\n{line2}\n{tool_url}"

    # keep clean
    low = reply.lower()
    for w in FORBIDDEN_REPLY_WORDS:
        if w in low:
            reply = reply.replace(w, "")
    # length guard
    if len(reply) > 430:
        reply = f"{summary}\nOne-page checklist here:\n{tool_url}"
    if len(reply) < 220:
        reply = f"{summary}\nI made a one-page checklist for this:\n{tool_url}"
    return reply.strip()


# =============================================================================
# Issues output (minimum 100 items; stub fill allowed)
# =============================================================================
def make_stub_posts(n: int) -> List[Post]:
    stubs: List[Post] = []
    for i in range(n):
        stubs.append(Post(
            source="stub",
            id=sha1(f"stub:{RUN_ID}:{i}"),
            url=f"{SITE_DOMAIN.rstrip('/')}/goliath/_out/stub/{RUN_ID}/{i}",
            text="Need a checklist / template for a common problem.",
            author="unknown",
            created_at=now_iso(),
        ))
    return stubs


def build_issue_items(posts: List[Post], post_to_tool_url: Dict[str, str]) -> List[Dict[str, str]]:
    items: List[Dict[str, str]] = []
    for p in posts:
        tool_url = post_to_tool_url.get(p.id, "")
        if not tool_url:
            continue
        reply = openai_generate_reply_stub(p, tool_url)
        items.append({
            "problem_url": p.url,
            "reply": reply,
            "source": p.source,
        })
    return items


def chunk_issue_bodies(items: List[Dict[str, str]], chunk_size: int = 40) -> List[str]:
    bodies: List[str] = []
    for i in range(0, len(items), chunk_size):
        chunk = items[i:i+chunk_size]
        lines: List[str] = []
        for it in chunk:
            lines.append(f"Problem URL: {it['problem_url']}")
            lines.append("Reply:")
            lines.append(it["reply"])
            lines.append("")  # blank
            lines.append("---")
        bodies.append("\n".join(lines).rstrip() + "\n")
    return bodies


def write_issues_payload(items: List[Dict[str, str]], extra_notes: str = "") -> str:
    """
    Write JSON with titles/bodies for GitHub issues.
    Returns path to JSON.
    """
    bodies = chunk_issue_bodies(items, ISSUE_MAX_ITEMS)
    payloads: List[Dict[str, str]] = []
    for idx, body in enumerate(bodies, start=1):
        title = f"Goliath reply candidates ({RUN_ID}) part {idx}/{len(bodies)}"
        if extra_notes and idx == 1:
            body = extra_notes.strip() + "\n\n" + body
        payloads.append({"title": title, "body": body})

    out_path = os.path.join(OUT_DIR, f"issues_payload_{RUN_ID}.json")
    write_json(out_path, {"run_id": RUN_ID, "count": len(items), "issues": payloads})
    return out_path


# =============================================================================
# Orchestration
# =============================================================================
def collect_all() -> List[Post]:
    # per spec targets:
    # Bluesky 50, Mastodon 100, Reddit 20, X 1(mentions), HN (rest)
    bs = collect_bluesky(max_items=50)
    ms = collect_mastodon(max_items=100)
    rd = collect_reddit(max_items=20)
    xx = collect_x_mentions(max_items=max(1, min(X_MAX, 5)))
    hn = collect_hn(max_items=HN_MAX)

    all_posts = bs + ms + rd + xx + hn
    # filter dup urls
    seen = set()
    out: List[Post] = []
    for p in all_posts:
        if not p.url:
            continue
        if adult_or_sensitive(p.text):
            continue
        key = sha1(p.url)
        if key in seen:
            continue
        seen.add(key)
        out.append(p)

    # cap MAX_COLLECT
    out = out[:MAX_COLLECT]
    logging.info("Collected total=%d (Bluesky=%d, Mastodon=%d, Reddit=%d, X=%d, HN=%d)",
                 len(out), len(bs), len(ms), len(rd), len(xx), len(hn))
    return out


def choose_themes(posts: List[Post], max_themes: int) -> List[Theme]:
    clusters = cluster_posts(posts, threshold=0.22)
    themes = [make_theme(c) for c in clusters if len(c) >= 2]
    themes.sort(key=lambda t: t.score, reverse=True)
    return themes[:max_themes]


def build_sites(themes: List[Theme], aff_norm: Dict[str, List[Dict[str, Any]]], all_sites_inventory: List[Dict[str, Any]], hero_bg_url: str) -> Tuple[List[Theme], List[Dict[str, Any]], Dict[str, str], List[str]]:
    """
    Builds pages + shortlinks. Updates site inventory list (not hub HTML).
    Returns:
      - built themes (with final slug + short_code)
      - new inventory list entries
      - mapping post_id -> tool_url for issue generation
      - list of urls for sitemap
    """
    os.makedirs(PAGES_DIR, exist_ok=True)
    os.makedirs(os.path.join(GOLIATH_DIR, "go"), exist_ok=True)

    sitemap_urls: List[str] = []
    new_inventory_entries: List[Dict[str, Any]] = []
    post_to_tool_url: Dict[str, str] = {}

    # compute popular once from current inventory
    popular_now = compute_popular_sites(all_sites_inventory, n=8)

    for theme in themes:
        # allocate collision-safe slug
        final_slug = allocate_unique_slug(theme.slug)
        theme.slug = final_slug

        tool_url = site_url_for_slug(final_slug)

        # shortlink
        code = short_code_for_url(tool_url)
        theme.short_code = code
        short_url = SITE_DOMAIN.rstrip("/") + f"/goliath/go/{code}/"

        # write shortlink page
        rel_path, short_html = build_shortlink_page(tool_url, code)
        abs_short_path = os.path.join(REPO_ROOT, rel_path)
        write_text(abs_short_path, short_html)

        # build content
        references = pick_reference_urls(theme)
        supplements = supplemental_resources_for_category(theme.category)[:max(SUPP_URL_MIN, 3)]
        article_ja = generate_long_article_ja(theme)
        faq = build_faq(theme.category)

        # affiliates top2
        aff_top2 = pick_affiliates_for_category(aff_norm, theme.category, topn=2)

        # related tools from existing inventory + new ones (accumulate)
        inventory_for_related = all_sites_inventory + new_inventory_entries
        related = choose_related_tools(inventory_for_related, theme.category, exclude_slug=final_slug, n=5)

        # build html
        html_text = build_page_html(
            theme=theme,
            tool_url=tool_url,
            short_url=short_url,
            affiliates_top2=aff_top2,
            references=references,
            supplements=supplements,
            article_ja=article_ja,
            faq=faq,
            related_tools=related,
            popular_sites=popular_now,
            hero_bg_url=hero_bg_url,
        )

        # validate/autofix
        attempts = 0
        errs = validate_site_html(html_text)
        while errs and attempts < MAX_AUTOFIX:
            attempts += 1
            # simple autofix: pad article, ensure markers
            if "article_maybe_too_short" in errs:
                article_ja = article_ja + "\n" + ("„ÄêËøΩÂä†„É°„É¢„Äë\n" + "Á¢∫Ë™ç‚ÜíÊúÄÂ∞èÂ§âÊõ¥‚ÜíÊ§úË®º‚ÜíË®òÈå≤„ÄÅ„ÅÆÈ†ÜÁï™„ÇíÂ¥©„Åï„Å™„ÅÑ„Åì„Å®„ÅåÊúÄÁü≠„Åß„Åô„ÄÇ\n") * 8
            # rebuild html after pad
            html_text = build_page_html(
                theme=theme,
                tool_url=tool_url,
                short_url=short_url,
                affiliates_top2=aff_top2,
                references=references,
                supplements=supplements,
                article_ja=article_ja,
                faq=faq,
                related_tools=related,
                popular_sites=popular_now,
                hero_bg_url=hero_bg_url,
            )
            errs = validate_site_html(html_text)

        if errs:
            logging.warning("Site validation still has errors for %s: %s", final_slug, errs)

        # write file
        out_dir = os.path.join(PAGES_DIR, final_slug)
        out_path = os.path.join(out_dir, "index.html")
        write_text(out_path, html_text)

        # sitemap urls
        sitemap_urls.append(tool_url)
        sitemap_urls.append(short_url)

        # inventory entry
        entry = {
            "slug": final_slug,
            "title": theme.title,
            "search_title": theme.search_title,
            "category": theme.category,
            "url": tool_url,
            "short_url": short_url,
            "created_at": now_iso(),
            "updated_at": now_iso(),
            "keywords": theme.keywords[:12],
        }
        new_inventory_entries.append(entry)

        # map representative posts to tool url (for issue output)
        for p in theme.representative_posts:
            post_to_tool_url[p.id] = tool_url

        logging.info("Built site: %s (%s) short=%s", tool_url, theme.category, short_url)

    return themes, new_inventory_entries, post_to_tool_url, sitemap_urls


def build_post_drafts(themes: List[Theme]) -> List[Dict[str, str]]:
    """
    Bluesky posting drafts: "one-line value" + short URL only (short).
    """
    drafts: List[Dict[str, str]] = []
    for t in themes:
        short_url = SITE_DOMAIN.rstrip("/") + f"/goliath/go/{t.short_code}/"
        one = short_value_line(t.category)
        # fixed format: value + URL
        text = f"{one}\n{short_url}"
        drafts.append({
            "category": t.category,
            "search_title": t.search_title,
            "short_url": short_url,
            "text": text,
        })
    return drafts


def write_run_summary(
    counts: Dict[str, int],
    reply_count: int,
    aff_audit: Dict[str, Any],
    post_drafts: List[Dict[str, str]],
    sitemap_url_written: str,
) -> None:
    """
    Self-check output (Actions log + json file).
    """
    summary = {
        "run_id": RUN_ID,
        "counts": counts,
        "reply_candidates": reply_count,
        "affiliates_audit": aff_audit,
        "sitemap_url": sitemap_url_written,
        "post_drafts": post_drafts,
        "updated_at": now_iso(),
    }
    out_path = os.path.join(OUT_DIR, f"run_summary_{RUN_ID}.json")
    write_json(out_path, summary)

    logging.info("Self-check: counts=%s", counts)
    logging.info("Self-check: reply_candidates=%d", reply_count)
    logging.info("Self-check: affiliates_audit ok=%s missing=%d extra=%d",
                 aff_audit.get("ok"), len(aff_audit.get("missing", [])), len(aff_audit.get("extra", [])))
    logging.info("Self-check: sitemap=%s", sitemap_url_written)


def main() -> int:
    setup_logging()

    # legal pages
    policy_urls = ensure_policies()

    # affiliates
    aff_raw = load_affiliates()
    aff_audit = audit_affiliate_keys(aff_raw)
    aff_norm = normalize_affiliates_shape(aff_raw)

    # collect
    posts = collect_all()
    counts = {
        "Bluesky": sum(1 for p in posts if p.source == "bluesky"),
        "Mastodon": sum(1 for p in posts if p.source == "mastodon"),
        "Reddit": sum(1 for p in posts if p.source == "reddit"),
        "X": sum(1 for p in posts if p.source == "x"),
        "HN": sum(1 for p in posts if p.source == "hn"),
        "Total": len(posts),
    }

    # choose themes
    themes = choose_themes(posts, max_themes=MAX_THEMES)
    if not themes:
        # ÂèéÈõÜ0„Åß„ÇÇÊúÄ‰Ωé1„Çµ„Ç§„ÉàÁîüÊàê
        seed_post = Post(
            source="seed",
            id=sha1(f"seed:{RUN_ID}"),
            url=HUB_BASE_URL.rstrip("/"),
            text="seed: no posts collected this run",
            author="system",
            created_at=now_iso(),
        )
        themes = [make_theme([seed_post])]
        logging.info("Chosen themes forced=1 (seed)")

    logging.info("Chosen themes=%d", len(themes))

    # hero background (optional)
    hero_bg = fetch_unsplash_bg_url()
    if hero_bg:
        logging.info("Unsplash hero bg enabled.")

    # inventory: read existing hub sites
    existing_sites = read_hub_sites()

    # build sites
    built_themes, new_entries, post_to_tool_url, site_urls = build_sites(
        themes=themes,
        aff_norm=aff_norm,
        all_sites_inventory=existing_sites,
        hero_bg_url=hero_bg,
    )

    # update hub/sites.json ONLY
    merged_sites = existing_sites + new_entries
    aggregates = compute_aggregates(merged_sites)
    write_hub_sites(merged_sites, aggregates)

    # Prepare reply candidates (minimum 100)
    mapped_post_ids = set(post_to_tool_url.keys())
    mapped_posts = [p for p in posts if p.id in mapped_post_ids]

    if len(mapped_posts) < LEADS_TOTAL:
        need = LEADS_TOTAL - len(mapped_posts)
        stubs = make_stub_posts(need)

        built_urls = [site_url_for_slug(t.slug) for t in built_themes] or [SITE_DOMAIN.rstrip("/") + "/hub/"]
        for i, sp in enumerate(stubs):
            post_to_tool_url[sp.id] = built_urls[i % len(built_urls)]

        mapped_posts.extend(stubs)

    mapped_posts = mapped_posts[: max(LEADS_TOTAL, 100)]

    issue_items = build_issue_items(mapped_posts, post_to_tool_url)

    if len(issue_items) < 100:
        more_need = 100 - len(issue_items)
        extra_stubs = make_stub_posts(more_need)
        built_urls = [site_url_for_slug(t.slug) for t in built_themes] or [SITE_DOMAIN.rstrip("/") + "/hub/"]
        for i, sp in enumerate(extra_stubs):
            post_to_tool_url[sp.id] = built_urls[i % len(built_urls)]
        issue_items.extend(build_issue_items(extra_stubs, post_to_tool_url))

    notes = []
    notes.append(f"Run: {RUN_ID}")
    notes.append(
        f"Collected: Bluesky={counts['Bluesky']} Mastodon={counts['Mastodon']} "
        f"Reddit={counts['Reddit']} X={counts['X']} HN={counts['HN']} Total={counts['Total']}"
    )
    notes.append(f"Reply candidates: {len(issue_items)}")
    if not aff_audit.get("ok"):
        notes.append("Affiliates audit: MISSING keys in affiliates.json:")
        for k in aff_audit.get("missing", []):
            notes.append(f"- {k}")
    if aff_audit.get("extra"):
        notes.append("Affiliates audit: EXTRA keys (ignored):")
        for k in aff_audit.get("extra", []):
            notes.append(f"- {k}")
    extra_notes = "\n".join(notes).strip()

    issues_path = write_issues_payload(issue_items, extra_notes=extra_notes)
    logging.info("Wrote issues payload: %s", issues_path)

    # post drafts (short URL + one-line value)
    drafts = build_post_drafts(built_themes)
    write_json(
        os.path.join(OUT_DIR, f"post_drafts_{RUN_ID}.json"),
        {"run_id": RUN_ID, "created_at": now_iso(), "drafts": drafts},
    )

    # sitemap + robots
    sitemap_urls = []
    sitemap_urls.extend(site_urls)
    sitemap_urls.extend(policy_urls)
    sitemap_urls.append(SITE_DOMAIN.rstrip("/") + "/hub/")

    sitemap_xml = build_sitemap(sitemap_urls)
    sitemap_out_path = os.path.join(OUT_DIR, "sitemap.xml")
    write_text(sitemap_out_path, sitemap_xml)

    sitemap_public_url = SITE_DOMAIN.rstrip("/") + "/sitemap.xml"
    robots_text = build_robots(sitemap_public_url)
    robots_out_path = os.path.join(OUT_DIR, "robots.txt")
    write_text(robots_out_path, robots_text)

    if ALLOW_ROOT_UPDATE:
        write_text(os.path.join(REPO_ROOT, "sitemap.xml"), sitemap_xml)
        write_text(os.path.join(REPO_ROOT, "robots.txt"), robots_text)
        logging.info("Root sitemap/robots updated.")
        if PING_SITEMAP:
            ping_search_engines(sitemap_public_url)
        sitemap_url_written = sitemap_public_url
    else:
        logging.info("Root sitemap/robots NOT updated (ALLOW_ROOT_UPDATE=0). Wrote to goliath/_out instead.")
        sitemap_url_written = SITE_DOMAIN.rstrip("/") + "/goliath/_out/sitemap.xml"

    # self-check summary
    write_run_summary(
        counts=counts,
        reply_count=len(issue_items),
        aff_audit=aff_audit,
        post_drafts=drafts,
        sitemap_url_written=sitemap_url_written,
    )

    return 0


if __name__ == "__main__":
    sys.exit(main())

    try:
        entry = (
            globals().get("main")
            or globals().get("run")
            or globals().get("run_goliath")
            or globals().get("goliath_main")
        )

        if not callable(entry):
            raise RuntimeError(
                "Entry function not found. Expected one of: "
                "main / run / run_goliath / goliath_main"
            )

        result = entry()
        if isinstance(result, int):
            sys.exit(result)

    except KeyboardInterrupt:
        raise
    except Exception as e:
        try:
            import logging
            logging.exception("Unhandled exception in goliath/main.py: %s", e)
        except Exception:
            print("Unhandled exception in goliath/main.py:", e, file=sys.stderr)
        raise
